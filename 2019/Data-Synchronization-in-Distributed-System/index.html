<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="分布式系统为保证数据高可用，需要为数据保存多个副本，随之而来的问题是如何在不同副本间同步数据？不同的同步机制有不同的效果和代价，本文尝试对常见分布式组件的同步机制做一个小结。"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>分布式系统常见同步机制 | 三点水</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/books">Books</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-noise-link" href="/tags/Elasticsearch/" rel="tag">Elasticsearch</a><a class="post-tag-noise-link" href="/tags/Kafka/" rel="tag">Kafka</a><a class="post-tag-noise-link" href="/tags/Redis/" rel="tag">Redis</a><a class="post-tag-noise-link" href="/tags/TiDB/" rel="tag">TiDB</a><a class="post-tag-noise-link" href="/tags/consistency/" rel="tag">consistency</a><a class="post-tag-noise-link" href="/tags/zookeeper/" rel="tag">zookeeper</a></div><div class="post-time">2019-08-21</div></div></div><div class="container post-header"><h1>分布式系统常见同步机制</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%88%B6"><span class="toc-number">1.</span> <span class="toc-text">常见机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#redis"><span class="toc-number">2.</span> <span class="toc-text">Redis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka"><span class="toc-number">3.</span> <span class="toc-text">Kafka</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#elasticsearch"><span class="toc-number">4.</span> <span class="toc-text">ElasticSearch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop"><span class="toc-number">5.</span> <span class="toc-text">Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tikv"><span class="toc-number">6.</span> <span class="toc-text">TiKV</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#zookeeper"><span class="toc-number">7.</span> <span class="toc-text">Zookeeper</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">9.</span> <span class="toc-text">参考</span></a></li></ol></details></div><div class="container post-content"><p>分布式系统为保证数据高可用，需要为数据保存多个副本，随之而来的问题是如何在不同副本间同步数据？不同的同步机制有不同的效果和代价，本文尝试对常见分布式组件的同步机制做一个小结。</p>
<h2 id="常见机制"><a class="header-anchor" href="#常见机制"></a>常见机制</h2>
<p>有一些常用的同步机制，对它们也有许多评价的维度，先看看大神的 <a href="https://snarfed.org/transactions_across_datacenters_io.html">经典总结
</a>：</p>
<img src="/2019/Data-Synchronization-in-Distributed-System/sync-trade-off.svg" class="" title="Sync Trade Off">
<p>上图给出了常用的同步方式（个人理解，请批评指正）：</p>
<ol>
<li>Backup，即定期备份，对现有的系统的性能基本没有影响，但节点宕机时只能勉强恢复</li>
<li>Master-Slave，主从复制，异步复制每个指令，可以看作是粒度更细的定期备份</li>
<li>Multi-Muster，多主，也称“主主”，MS 的加强版，可以在多个节点上写，事后再想办法同步</li>
<li>2 Phase-Commit，二阶段提交，同步先确保通知到所有节点再写入，性能容易卡在“主”节点上</li>
<li>Paxos，类似 2PC，同一时刻有多个节点可以写入，也只需要通知到大多数节点，有更高的吞吐</li>
</ol>
<p>同步方式分两类，异步的性能好但可能有数据丢失，同步的能保证不丢数据但性能较差。同种方式的算法也能有所提升（如 Paxos 对于 2PC），但实现的难度又很高。实现上只能在这几点上进行权衡。</p>
<p>考虑同步算法时，需要考虑节点宕机、网络阻断等故障情形。下面，我们来看看一些分布式组件的数据同步机制，主要考虑数据写入请求如何被处理，期间可能会涉及如何读数据。</p>
<h2 id="redis"><a class="header-anchor" href="#redis"></a>Redis</h2>
<p>Redis 3.0 开始引入 Redis Cluster 支持集群模式，个人认为它的设计很漂亮，大家可以看看<a href="https://redis.io/topics/cluster-spec">官方文档</a>。</p>
<ul>
<li>采用的是主从复制，异步同步消息，极端情况会丢数据</li>
<li>只能从主节点读写数据，从节点只会拒绝并让客户端重定向，不会转发请求</li>
<li>如果主节点宕机一段时间，从节点中会自动选主</li>
<li>如果期间有数据不一致，以最新选出的主节点的数据为准。</li>
</ul>
<p>一些设计细节：</p>
<ul>
<li>Redis 的 Key 会被分配(分片/分桶？)到 16384 个 slot 中，每个节点提供部分 slot 的数据</li>
<li>分配的算法为 <code>HASH_SLOT = CRC16(Key) mod 16384</code></li>
<li>集群的节有一个随机生成的唯一 ID，节点的 IP 可以变，但 ID 不会变</li>
<li>新节点加入时先执行 <code>MEET</code> 来认识集群中的某个节点，集群节点间相互“八卦(gossip)”，最终相互认识</li>
<li>主从的粒度是节点，不是 slot。</li>
<li>自动选主，使用类似 Raft 的选主机制。</li>
<li>也提供了 <code>WAIT</code> 指令来来保证写入时同步复制到从节点。</li>
</ul>
<h2 id="kafka"><a class="header-anchor" href="#kafka"></a>Kafka</h2>
<p>Kafka 的分片粒度是 Partition，每个 Partition 可以有多个副本。副本同步设计参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication">
官方文档</a></p>
<ul>
<li>类似于 2PC<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，节点分主从，同步更新消息，除非节点全挂，否则不会丢消息</li>
<li>消息发到主节点，主节点写入后等待“所有”从节点拉取该消息，之后通知客户端写入完成</li>
<li>“所有”节点指的是 In-Sync Replica(ISR)，响应太慢或宕机的从节点会被踢除</li>
<li>主节点宕机后，从节点选举成为新的主节点，继续提供服务</li>
<li>主节点宕机时正在提交的修改没有做保证（消息可能没有 ACK 却提交了<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>）</li>
</ul>
<p>一些设计细节：</p>
<ul>
<li>当前消费者只能从主节点读取数据，未来可能会改变<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></li>
<li>主从的粒度是 partition，每个 broker 对于某些 Partition 而言是主节点，对于另一些而言是从节点</li>
<li>Partition 创建时，Kafka 会尽量让 preferred replica 均匀分布在各个 broker</li>
<li>选主由一个 controller 跟 zookeeper 交互后“内定”，再通过 RPC 通知具体的主节点，此举能防止 partition 过多，同时选主导致 zk 过载。</li>
</ul>
<h2 id="elasticsearch"><a class="header-anchor" href="#elasticsearch"></a>ElasticSearch</h2>
<p>ElasticSearch 对数据的存储需求和 Kafka 很类似，设计也很类似，详细可见<a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/distributed-docs.html">官方文档</a>。</p>
<p>ES 中有 master node 的概念，它实际的作用是对集群状态进行管理，跟数据的请求无关。为了上下文一致性，我们称它为管理节点，而称 primary shard 为“主节点”， 称
replica shard 为从节点。ES 的设计：</p>
<ul>
<li>类似于 2PC<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>，节点分主从，同步更新消息，除非节点全挂，否则不会丢消息</li>
<li>消息发到主节点，主节点写入成功后并行发给从节点，等到从节点全部写入成功，通知客户端写入完成</li>
<li>管理节点会维护每个分片需要写入的从节点列表，称为 in-sync copies</li>
<li>主节点宕机后，从节点选举成为新的主节点，继续提供服务</li>
<li>提交阶段从节点不可用的话，主节点会要求管理节点将从节点从 in-sync copies 中移除</li>
</ul>
<p>一些设计细节：</p>
<ul>
<li>写入只能通过只主节点进行，读取可以从任意从节点进行<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></li>
<li>每个节点均可提供服务，它们会转发请求到数据分片所在的节点，但建议循环访问各个节点以平衡负载</li>
<li>数据做分片：<code>shard = hash(routing) % number_of_primary_shards</code></li>
<li>primary shard 的数量是需要在创建 index 的时候就确定好的</li>
<li>主从的粒度是 shard，每个节点对于某些 shard 而言是主节点，对于另一些而言是从节点</li>
<li>选主算法使用了 ES 自己的 Zen Discovery<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li>
</ul>
<h2 id="hadoop"><a class="header-anchor" href="#hadoop"></a>Hadoop</h2>
<p>Hadoop 使用的是链式复制，参考
<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Replication_Pipelining">Replication Pipelining</a></p>
<ul>
<li>数据的多个复本写入多个 datanode，只要有一个存活数据就不会丢失</li>
<li>数据拆分成多个 block，每个 block 由 namenode 决定数据写入哪几个 datanode</li>
<li>链式复制要求数据发往一个节点，该节点发往下一节点，待下个节点返回及本地写入成功后返回，以此类推形成一条写入链。</li>
<li>写入过程中的宕机节点会被移除 pineline，不一致的数据之后由 namenode 处理。</li>
</ul>
<p>实现细节：</p>
<ul>
<li>实现中优化了链式复制：block 拆分成多个 packet，节点 1 收到 packet, 写入本地的同时发往节点 2，等待节点 2 完成及本地完成后返回 ACK。节点 2 以此类推将
packet 写入本地及发往节点 3……</li>
</ul>
<h2 id="tikv"><a class="header-anchor" href="#tikv"></a>TiKV</h2>
<p>TiKV 使用的是 Raft 协议来实现写入数据时的一致性。参考 <a href="https://zhuanlan.zhihu.com/p/26967545">三篇文章了解 TiDB 技术内幕——说存储</a></p>
<ul>
<li>使用 Raft，写入时需要半数以上的节点写入成功才返回，宕机节点不超过半数则数据不丢失。</li>
<li>TiKV 将数据的 key 按 range 分成 region，写入时以 region 为粒度进行同步。</li>
<li>写入和读取都通过 leader 进行。每个 region 形成自己的 raft group，有自己的 leader。</li>
</ul>
<h2 id="zookeeper"><a class="header-anchor" href="#zookeeper"></a>Zookeeper</h2>
<p>Zookeeper 使用的是 Zookeeper 自己的 Zab 算法(Paxos 的变种？)，参考 <a href="https://zookeeper.apache.org/doc/r3.5.5/zookeeperInternals.html">Zookeeper
Internals</a></p>
<ul>
<li>数据只可以通过主节点写入（请求会被转发到主节点进行），可以通过任意节点读取</li>
<li>主节点写入数据后会广播给所有节点，超过半数节点写入后返回客户端</li>
<li>Zookeeper 不保证数据读取为最新，但通过“单一视图”保证读取的数据版本不“回退”</li>
</ul>
<h2 id="小结"><a class="header-anchor" href="#小结"></a>小结</h2>
<p>如果系统对性能要求高以至于能容忍数据的丢失(Redis)，则显然异步的同步方式是一种好的选择。</p>
<p>而当系统要保证不丢数据，则几乎只能使用同步复制的机制，看到 Kafka 和
Elasticsearch 不约而同地使用了 PacificA 算法（个人认为可以看成是 2PC 的变种），当然这种方法的响应制约于最慢的副本，因此 Kafka 和 Elasticsearch 都有相关的机制将慢的副本移除。</p>
<p>当然看起来 Paxos, Raft, Zab 等新的算法比起 2PC 还是要好的：一致性保证更强，只要半数节点写入成功就可以返回，Paxos 还支持多点写入。只不过这些算法也很难正确实现和优化。</p>
<h2 id="参考"><a class="header-anchor" href="#参考"></a>参考</h2>
<ul>
<li><a href="http://note.yuhc.me/2015/12/paxos-made-simple/">http://note.yuhc.me/2015/12/paxos-made-simple/</a> Paxos 与 2PC/3PC 的对比</li>
<li><a href="https://kafka.apache.org/documentation/#design_replicatedlog">https://kafka.apache.org/documentation/#design_replicatedlog</a> Kafka
Replicated Log 机制说明</li>
<li><a href="https://www.confluent.io/blog/hands-free-kafka-replication-a-lesson-in-operational-simplicity">Hands-free Kafka Replication: A lesson in operational simplicity</a> Kafka ISR 的内容很值得参考</li>
<li><a href="http://www.jasongj.com/2015/04/24/KafkaColumn2/">http://www.jasongj.com/2015/04/24/KafkaColumn2/</a> Kafka HA 设计</li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6980045.html">https://www.cnblogs.com/huxi2b/p/6980045.html</a> Kafka controller 设计</li>
<li><a href="http://cloudurable.com/blog/kafka-architecture-low-level/index.html">http://cloudurable.com/blog/kafka-architecture-low-level/index.html</a> Kafka 底层设计</li>
<li><a href="https://www.elastic.co/blog/found-leader-election-in-general">https://www.elastic.co/blog/found-leader-election-in-general</a> ES 选主机制</li>
<li><a href="https://www.elastic.co/blog/tracking-in-sync-shard-copies">https://www.elastic.co/blog/tracking-in-sync-shard-copies</a> ES Shard Copies 同步机制</li>
<li><a href="https://medium.com/@Alibaba_Cloud/elasticsearch-distributed-consistency-principles-analysis-3-data-a98cc436bc6b">Elasticsearch Distributed Consistency Principles Analysis (3) — Data</a> ES 同步机制的分析</li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2008/02/tr-2008-25.pdf">PacificA: Replication in Log-Based Distributed Storage System</a> PacificA 算法论文</li>
<li><a href="https://sq.163yun.com/blog/article/172841852382142464">分布式文件系统之一致性复制协议</a> 常用的复制协议，包括 PacificA 算法</li>
<li><a href="http://www.distorage.com/%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%8D%E5%88%B6%E6%8A%80%E6%9C%AF/">分布式复制技术</a> 讲解了链式复制、树形复制、分发复制</li>
<li><a href="https://bigdataafterlife.wordpress.com/tutorials/write-operations-in-hdfs/">Write operations in HDFS</a> 解释 HDFS 写入操作</li>
<li><a href="https://www.percona.com/live/17/sites/default/files/slides/A%20brief%20introduction%20of%20TiDB%20%28Percona%20Live%29.pdf">A Brief Introduction of TiDB</a> TiDB 架构介绍 PPT</li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://kafka.apache.org/10/documentation.html">官文文档</a>
提到 Kafka 使用 PacificA 算法 <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://jack-vanlightly.com/blog/2018/9/14/how-to-lose-messages-on-a-kafka-cluster-part1">How to Lose Messages on a Kafka Cluster - Part
1</a>
实验中 <code>ack=all</code> 时即使没有 ACK 的数据也会写入 <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-392:+Allow+consumers+to+fetch+from+closest+replica">KIP-392: Allow consumers to fetch from closest
replica</a>
Kafka 提议消费者可以从从节点读取数据 <a href="#fnref3" class="footnote-backref">↩</a></p>
</li>
<li id="fn4" class="footnote-item"><p>更准确地说，ES 的同步算法<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html">是 PacificA 算法的不完全实现</a> <a href="#fnref4" class="footnote-backref">↩</a></p>
</li>
<li id="fn5" class="footnote-item"><p>实际上这导致 ES 无法保证顺序一致性，你可能读到新版的数据后又读到旧版的数据 <a href="#fnref5" class="footnote-backref">↩</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://www.elastic.co/blog/a-new-era-for-cluster-coordination-in-elasticsearch">A new era for cluster coordination in
Elasticsearch</a> ES 7 之后算法有变化 <a href="#fnref6" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</div></div><div class="post-main post-comment"><div id="giscus_thread"></div><script src="https://giscus.app/client.js" data-repo="lotabout/lotabout.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkyMDU1NTQ0Nw==" data-category="Announcements" data-category-id="DIC_kwDOATmmt84ClmcD" data-mapping="" data-strict="" data-reactions-enabled="0" data-emit-metadata="" data-input-position="" data-theme="" data-lang="zh-CN" data-loading="" crossorigin="" async>
</script></div></article><link rel="stylesheet" type="text/css" href="/css/third-party/font-awesome/4.5.0/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/fancybox/2.1.5/jquery.fancybox.css"><script src="/js/third-party/jquery/2.0.3/jquery.min.js"></script><script src="/js/third-party/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=#{theme.google_analytics}"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-39956831-2');</script></body></html>