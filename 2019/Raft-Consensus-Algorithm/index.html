<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="分布式系统中，如何保证多个节点的状态一致？Raft 一致性算法与 Paxos 不同，号称简单易学，且已经广泛应用在生产中。例如 k8s 和 CoreOS 中使用的 etcd；tikv 中使用
Raft 完成分布式同步；Redis Cluster 中使用类似 Raft 的选主机制等等。今天我们来一探究竟吧。"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>Raft 一致性算法 | 三点水</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/books">Books</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-noise-link" href="/tags/Algorithm/" rel="tag">Algorithm</a><a class="post-tag-noise-link" href="/tags/Consensus/" rel="tag">Consensus</a><a class="post-tag-noise-link" href="/tags/Raft/" rel="tag">Raft</a></div><div class="post-time">2019-05-16</div></div></div><div class="container post-header"><h1>Raft 一致性算法</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA-replicated-state-machines"><span class="toc-number">1.</span> <span class="toc-text">复制状态机&#x2F;Replicated state machines</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#raft-%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">Raft 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.</span> <span class="toc-text">基础概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E4%B8%BB-leader-election"><span class="toc-number">2.2.</span> <span class="toc-text">选主&#x2F;Leader Election</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6-log-replication"><span class="toc-number">2.3.</span> <span class="toc-text">日志复制&#x2F;Log Replication</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E5%85%A8%E6%80%A7"><span class="toc-number">2.4.</span> <span class="toc-text">安全性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%89%E4%B8%BB%E7%9A%84%E9%99%90%E5%88%B6"><span class="toc-number">2.4.1.</span> <span class="toc-text">选主的限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4%E5%89%8D%E4%B8%80%E4%B8%AA-term-%E7%9A%84%E6%97%A5%E5%BF%97"><span class="toc-number">2.4.2.</span> <span class="toc-text">提交前一个 term 的日志</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E5%85%A8%E6%80%A7%E8%AF%B4%E6%98%8E"><span class="toc-number">2.4.3.</span> <span class="toc-text">安全性说明</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#follower-%E4%B8%8E-candidate-%E5%AE%95%E6%9C%BA"><span class="toc-number">2.5.</span> <span class="toc-text">Follower 与 Candidate 宕机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="toc-number">2.6.</span> <span class="toc-text">算法伪代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4"><span class="toc-number">3.</span> <span class="toc-text">成员变更</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%A4%9A%E6%95%B0%E6%B4%BE%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.</span> <span class="toc-text">双多数派问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">成员变更算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81-v2"><span class="toc-number">3.3.</span> <span class="toc-text">算法伪代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">4.</span> <span class="toc-text">参考</span></a></li></ol></details></div><div class="container post-content"><p>分布式系统中，如何保证多个节点的状态一致？Raft 一致性算法与 Paxos 不同，号称简单易学，且已经广泛应用在生产中。例如 k8s 和 CoreOS 中使用的 etcd；tikv 中使用
Raft 完成分布式同步；Redis Cluster 中使用类似 Raft 的选主机制等等。今天我们来一探究竟吧。</p>
<h2 id="复制状态机-replicated-state-machines"><a class="header-anchor" href="#复制状态机-replicated-state-machines"></a>复制状态机/Replicated state machines</h2>
<p>复制状态机的想法是将服务器看成一个状态机，而一致性算法的目的是让多台服务器/状态机能够计算得到相同的状态，同时，如果有部分机器宕机，集群作为一个整体依然能继续工作。复制状态机一般通过复制日志（replicated log）来实现，如下图：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-replicated-state-machine.svg" class="" title="Replicated State Machine">
<p>服务器会将客户端发来的命令存成日志，日志是有序的。而服务器状态机执行命令的结果是确定的，这样如果每台服务器的状态机执行的命令是相同的，状态机最终的状态也会是相同的，输出的结果也会是相同的。而如何保证不同服务器间的日志是一样的呢？这就是其中的“一致性模块”的工作了。</p>
<p>一致性模块（consensus module）在收到客户端的命令时（②），一方面要将命令添加到自己的日志队列中，同时需要与其它服务器的一致性模块沟通，确保所有的服务器将<strong>最终</strong>拥有相同的日志，即使有些服务器可能挂了。实践中至少需要“大多数(大于一半)”服务器同步了命令才认为同步成功了。</p>
<h2 id="raft-算法"><a class="header-anchor" href="#raft-算法"></a>Raft 算法</h2>
<p>接下去会从 3 个方面讲解 Raft 算法：</p>
<ol>
<li>选主（Leader Election）。Raft 在同一时刻只有一个主节点能接收写命令。</li>
<li>日志复制（Log Replication）。Raft 如何将接收到的命令复制到其它服务器上，使其保持一致？</li>
<li>安全性，为什么 Raft 在各种情况下依旧能保证各服务器的日志一致性？</li>
</ol>
<h3 id="基础概念"><a class="header-anchor" href="#基础概念"></a>基础概念</h3>
<p>首先是节点的 3 种状态/角色：</p>
<ul>
<li>Follower/从节点不发起请求，单纯响应 Candidate 和 Leader 的请求</li>
<li>Leader/主节点负责响应客户端发起的请求，如果客户端的请求发送到 Follower，请求会被转发到主节点</li>
<li>Candidate/候选节点是一种中间状态，只在选主期间存在。</li>
</ul>
<p>它们的转换关系如下：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-server-states.svg" class="" title="Server States">
<p>其次是任期（term）的概念。Raft 将时间切分成多个 term，每个 term 以选主开始，选主期间各节点尝试当上主节点，选举结束后开始正常处理客户端的请求。如图：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-term-definition.svg" class="" title="Term Definition">
<p>Raft 会保证每一个 term 中至多只有一个 leader，如果选主时选票被分散导致没有节点获得多数票（如 t3），则会开始新一轮选举。</p>
<p>term 就像逻辑上的“时间”，用来记录和比较各节点的“进度”。如果某个节点收到信息时发现自己的 term 是落后的，它会立即将自己的 term 更新为更大的 term；同时节点不会理睬 term 比自己小的消息；另外如果主节点收到 term 比自己大的消息，则会立马进入 follower 的状态。</p>
<p>例如由于网络情况不佳，一个主节点 A 与其它节点失联，其它节点选了一个新的主节点
B，当网络恢复正常时，旧主节点 A 收到主节点 B 的消息时，它会判断新主节点 B 的
term 大于自己，说明自己错过了一些事件，因此选择放弃自己的主节点身份。</p>
<h3 id="选主-leader-election"><a class="header-anchor" href="#选主-leader-election"></a>选主/Leader Election</h3>
<p>节点启动时，默认处于 Follower 的状态，所以开始时所有节点均是 Follower，那么什么时候触发选主呢？Raft 用“心跳”的方式来保持主从节点的联系，如果长时间没有收到主节点的心跳，则开始选主。这里会涉及到两个时间：</p>
<ul>
<li>心跳间隔，主节点隔多长时间发送心跳信息</li>
<li>等待时间(election timeout)，如果超过这个时间仍然没有收到心跳，则认为主节点宕机。一般每个节点各自在 150～300ms 间随机取值。</li>
</ul>
<p>当一个节点在等待时间内没有收到主节点的心跳信息，它首先将自己保存的 term 增加
<code>1</code> 并进入 Candidate 状态。此时它会先投票给自己，然后并行发送 <code>RequestVote</code>消息给其它所有节点，请求这些节点投票给自己。然后等待直到以下 3 种情形之一发生：</p>
<ol>
<li>收到大于一半的票，当选为主节点</li>
<li>有其它节点当选了主节点，此时会收到新的主节点的心跳</li>
<li>过了一段时间后依旧没有当选，此时该节点会尝试开始新一轮选举</li>
</ol>
<p>对于第一种情形，Candidate 节点需要收到集群中与自己 term 相同的所有节点中大于一半的票数（当然如果节点 term 比自己大，是不会理睬自己的选举消息的）。节点投票时会采取先到先得的原则，对于某个 term，最多投出一票（后面还会再对投票加一些限制）。这样能保证某个 term 中，最多只会产生一个 leader。当一个 Candidate 变成主节点后，它会向其它所有节点发送心跳信息，这样其它的 Candidate 也会变成 Follower。</p>
<p>第二种情形是在等待投票的过程中，Candidate 收到其它主节点的心跳信息（只有主节点才会向其它节点发心跳），且信息中包含的 term 大于等于自己的 term，则当前节点放弃竞选，进入 Follower 状态。当然，如前所说，如果心跳中的 term 小于自己，则不予理会。</p>
<p>第三种情形一般发生在多个 Follower 同时触发选举，而各节点的投票被分散了，导致没有 Candidate 能得到多数票。超过投票的等待时间后，节点触发新一轮选举。理论上，选举有可能永远平票，Raft 中由于各个节点的超时时间是随机的，实际上平票不太会永远持续下去。</p>
<h3 id="日志复制-log-replication"><a class="header-anchor" href="#日志复制-log-replication"></a>日志复制/Log Replication</h3>
<p>Log Replication 分为两个主要步骤：复制/Replication 和 提交/Commit。当一个节点被选为主节点后，它开始对外提供服务，收到客户端的 command 后，主节点会首先将
command 添加到自己的日志队列中，然后并行地将消息发送给其它所有的节点，在确保消息被安全地复制（下文解释）后，主节点会将该消息提交到状态机中，并返回状态机执行的结果。如果follower 挂了或因为网络原因消息丢失了，主节点会不断重试直到所有从节点<strong>最终</strong>成功复制该消息。</p>
<p>日志结构示例如下：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-log-entries.svg" class="" title="Log Entries">
<p>日志由许多条目（log entry）组成，条目顺序编号。条目包含它生成时节点所在的 term
（小方格中上方的数字），以及日志的内容。当一个条目被认为安全地被复制，且提交到状态机时，我们认为它处于“已提交（committed）”状态。</p>
<p>是否将一个条目提交到状态机是由主节点决定的。Raft 要保证提交的条目会<strong>最终</strong>被所有的节点执行。当主节点判断一个条目已经被复制到<strong>大多数</strong>节点时，就会<strong>提交
/Commit</strong>该条目，提交一个条目的同时会提交该条目之前的所有条目，包括那些之前由其它主节点创建的条目（还有些特殊情况下面会提）。主节点会记录当前提交的日志编号
(log index)，并在发送心跳时带上该信息，这样其它节点最终会同步提交日志。</p>
<p>上面说的是“提交”，那么“复制”是如何进行的？在现实情况下，主从节点的日志可能不一致（例如在消息到达从节点前主节点挂了，而从节点被选为了新的主节点，此时主从节点的日志不一致）。Raft 算法中，主节点需要处理不一致的情况，它要求所有的从节点复制自己的所有日志（当然下一小节会介绍额外的限制，保证复制是安全的）。</p>
<p>要复制所有日志，就要先找到日志开始不一致的位置，如何做到呢？Raft 当主节点接收到新的 command 时，会发送 <code>AppendEntries</code> 让从节点复制日志，不一致的情况也会在这时被处理（<code>AppendEntries</code> 消息同时还兼职作为心跳信息）。下面是日志不一致的示例：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-log-inconsistency.svg" class="" title="Log Inconsitency">
<p>主节点需要为每个从节点记录一个 <code>nextIndex</code>，作为该从节点下一条要发送的日志的编号。当一个节点刚被选为主节点时，为所有从节点的 <code>nextIndex</code> 初始化自己最大日志编号加 1（如上图示例则为 <code>11</code>）。接着主节点发送 <code>AppendEntries</code> 给从节点，此时从节点会进行一致性检查（Consistency Check）。</p>
<p>所谓一致性检查，指的是当主节点发送 <code>AppendEntries</code> 消息通知从节点添加条目时，需要将新条目 A 之前的那个条目 B 的 log index 和 term，这样，当从节点收到消息时，就可以判断自己第log index 条日志的 term 是否与 B 的 term 相同，如果不相同则拒绝该消息，如果相同则添加条目 A。</p>
<p>主节点的消息被某个从节点拒绝后，主节点会将该从节点的 <code>nextIndex</code> 减一再重新发送<code>AppendEntries</code> 消息。不断重试，最终就能找主从节点日志一致的 log index，并用主节点的新日志覆盖从节点的旧日志。当然，如果从节点接收 <code>AppendEntries</code> 消息后，主节点会将 <code>nextIndex</code> 增加一，且如果当前的最新 log index 大于
<code>nextIndex</code> 则会继续发送消息。</p>
<p>通过以上的机制，Raft 就能保证：</p>
<ul>
<li>如果两个日志条目有相同的 log index 和 term，则它们的内容一定相同。</li>
<li>如果两个节点中的两个条目有相同的 log index 和 term，则它们之前的所有日志一定相同。</li>
</ul>
<h3 id="安全性"><a class="header-anchor" href="#安全性"></a>安全性</h3>
<p>要保证所有的状态机有一样的状态，单凭前几节的算法还不够。例如有 3 个节点 A、B、
C，如果 A 为主节点期间 C 挂了，此时消息被多数节点（A，B）接收，所以 A 会提交这些日志。此时若 A 挂了，而 C 恢复且被选为主节点，则 A 已经提交的日志会被 C 的日志覆盖，从而导致状态机的状态不一致。</p>
<h4 id="选主的限制"><a class="header-anchor" href="#选主的限制"></a>选主的限制</h4>
<p>在所有的主从结构的一致性算法中，主节点最终都必须包含所有提交的日志。有些算法在从节点不包含所有已提交日志的情况下，依旧允许它当选为主节点，之后从节点会将这些日志同步到主节点上。但是 Raft 采用了简单的方式，只允许那些包含所有已提交日志的节点当选为主节点。</p>
<p>注意到节点当选主节点要求得到<strong>多数</strong>票，同时一个日志被提交的前提条件是它被<strong>多数</strong>节点接收，综合这两点，说明选举要产生结果，则至少有一个节点在场，它是包含了当前已经提交的所有日志的。</p>
<p>因此，Raft 算法在处理要求选举的 <code>RequestVote</code> 消息时做了限制：消息中会携带
Candidate 的 log 消息，而在投票时，Follower 会判断 Candidate 的消息是不是比自己“更新”（下文定义），如果不如自己“新”，则拒绝为该 Candidate 投票。</p>
<p>Raft 会首先判断两个节点最后一个 log entry 的 term，哪个节点的对应的 term 更大则代表该节点的日志“更新”；如果 term 的大小一致，则谁的 log entry 更多谁就“更新”。</p>
<p>注意，加了这个限制后，选出的节点不会是“最新的”，即包含所有日志；但会是足够新的，至少比半数节点更新，而这也意味着它所包含的日志都是可以被提交的（但不一定已经提交）。</p>
<h4 id="提交前一个-term-的日志"><a class="header-anchor" href="#提交前一个-term-的日志"></a>提交前一个 term 的日志</h4>
<p>这里我们要讨论一个特别的情况。我们知道一个主节点如果发现自己任期（term）内的某条日志已经被存储到了多数节点上，主节点就会提交这条日志。但如果主节点在提交之前就挂了，之后的主节点会尝试把前任未提交的这些日志复制到所有子节点上，但与之前不同，仅仅判断这些日志被复制到多数节点，新的主节点并不能立马提交这些日志，下面举一个反例：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-log-safety.svg" class="" title="Log Safety Example">
<p>在 <code>(a)</code> 时，S1 当选并将日志编号为 <code>2</code> 的日志复制到其它节点上。在 <code>(b)</code> 时，S1
宕机，S5 获得来自 S3 与 S4 的投票，当选为 term <code>3</code> 的主节点，此时收到来自客户端的消息，写入自己编号为 <code>2</code> 的日志。<code>(c)</code> 期间，S5 宕机而 S1 重启完毕，它重新当选为主节点并继续将自己的日志复制给 S3，此时编号为 2 且 term 为 2 的日志已经被复制到多数节点，但它还不能被提交。如果此时 S1 宕机，如 <code>(d)</code> 所示，此时 S5 获得来自 S2 S3 S4 的投票，当选新的主节点，此时它将用自己的编号为 2，term 为 3 的日志覆盖其它节点的日志。而如果 S1 继续存活，且在自己的任期内将某条日志复制到多数节点，如 <code>(e)</code> 所示，则此时 S5 已经不可能继续当选为主节点，因此该日志之前的所有日志均可被提交（包括前任创建的，编号 2 的日志）。</p>
<p>上例中的 <code>(c)</code> 和 <code>(d)</code> 说明了，即使前任的日志已经被复制到多数节点上，它依然可能被覆盖。因此 Raft 并<strong>不通过</strong>计算前任日志的复制次数来判断是否提交这些日志，
Raft 只对自己任期内的日志计数并在复制到多数节点时进行提交，且在提交这条日志的同时提交之前的所有日志。</p>
<p>Raft 算法会出现这个额外的问题，是因为它在复制前任的日志时，会保留前任的 term，而其它一致性算法会为这些日志使用新的 term。Raft 算法的优势在于方便推理日志的形成过程，同时新的主节点需要发送的前任日志数目会更少。</p>
<h4 id="安全性说明"><a class="header-anchor" href="#安全性说明"></a>安全性说明</h4>
<p>Raft 算法的安全性是经过理论证明的，这部分博主不熟悉相关领域，只得请大家自行看原论文了。</p>
<h3 id="follower-与-candidate-宕机"><a class="header-anchor" href="#follower-与-candidate-宕机"></a>Follower 与 Candidate 宕机</h3>
<p>这部分 Raft 的处理非常简单，如果 Follower 或 Candidate 宕机，主节点会不断进行重试，即不管挂不挂都照常发送 <code>AppendEntries</code> 消息。这样当 Follower 或
Candidate 恢复之后，日志仍能被正确复制。有时 Follower 会处理消息却在响应前宕机，此时由于 Raft 算法是幂等的，因此重复发送也没有关系。</p>
<h3 id="算法伪代码"><a class="header-anchor" href="#算法伪代码"></a>算法伪代码</h3>
<p>下图来源于原论文：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-algorithm.svg" class="full" title="Raft Algorithm">
<h2 id="成员变更"><a class="header-anchor" href="#成员变更"></a>成员变更</h2>
<p>假设已经有了一个 Raft 集群，现在要往集群中增加/移除若干个节点，要如何实现？</p>
<h3 id="双多数派问题"><a class="header-anchor" href="#双多数派问题"></a>双多数派问题</h3>
<p>一种方法是先停止所有节点，修改配置增加新的节点，再重启所有节点，但是这样服务起停时就会中断服务，同时也可能增加人为操作失误的风险。另一种方法配置好新的节点直接加入集群，这样也会出问题：在某个时刻使用不同配置的两部分节点可能会各自选出一个主节点。如下图：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-disjoint-majorities.svg" class="" title="Two disjoint Majorities Problem">
<p>图中绿色为旧的配置，蓝色为新的配置，在中间的某个时刻，Server 1/2/3 可能会选出一个主节点，而 Server 3/4/5 可能会选出另一个，从而破坏了一致性。</p>
<h3 id="成员变更算法"><a class="header-anchor" href="#成员变更算法"></a>成员变更算法</h3>
<p><a href="https://raft.github.io/raft.pdf">原版论文</a> 提出了一个比较复杂的算法，利用一个中间的过度状态来从 C_old 过度到 C_new （这里的 C_xxx 指的是成员的配置），在作者的<a href="https://github.com/ongardie/dissertation">博士论文</a>中指出了一个更简单的方法。作者发现，如果一次只增加或减少一个节点，那么并不会出现上面说的两个多数派的问题。示例如下：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-add-delete-single-node.svg" class="" title="Add Or Remove A Single Node">
<p>上图的 4 种情况分别代表原始节点数为奇数和偶数的情况下，添加或移除一个节点时可能产生的“大多数节点”的分组情况。注意到所有的分组都至少会一个节点会出现在两个分组中，那么，如果该节点是主节点，则其它所有节点均不可能当选主节点；如果该节点不是主节点，则它至少应该投票给其中一个分组中的其它节点，但这样一来另一个分组就达到不到票数来产生新的主节点了。因此在只增加/减少一个节点的情况下，不可能同时产生两个主节点。</p>
<p>当主节点收到对当前集群（C_old）新增/移除节点的请求时，它会将新的集群配置（C_new）作为一条新的日志加入到队列中，并用上文提到的机制复制到其它各个节点。当一个节点收到新的日志时，日志中的 C_new 会立即生效，即该节点的日志会被复制到 C_new 中配置的其它节点，且日志是否被提交也以 C_new 中指定的节点作为依据。这意味着节点不需要等 C_new 日志被提交后才开始启用 C_new，且每个节点总是使用它的日志中最新的配置。</p>
<p>当主节点提交 C_new 日志后，新增/移除节点的操作就算结束。此时，主节点能确定至少
C_new 中的多数节点已经启用了 C_new 配置，同时，那些还没有启用 C_new 的节点也不再可能组成新的“多数节点”。</p>
<h3 id="算法伪代码-v2"><a class="header-anchor" href="#算法伪代码-v2"></a>算法伪代码</h3>
<p>下图来源于原论文：</p>
<img src="/2019/Raft-Consensus-Algorithm/raft-membership-algorithm.svg" class="full" title="Two disjoint Majorities Problem">
<h2 id="参考"><a class="header-anchor" href="#参考"></a>参考</h2>
<ul>
<li><a href="https://raft.github.io/raft.pdf">https://raft.github.io/raft.pdf</a> Raft 原论文</li>
<li><a href="https://github.com/ongardie/dissertation">https://github.com/ongardie/dissertation</a> 作者的博士论文，每个主题的更详细描述</li>
<li><a href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a> 一个动画版的教程</li>
<li><a href="https://www.yangcs.net/posts/etcd-server-learner/">通过集群成员变更来看 etcd 的分布式一致性</a>
对 etcd 相关机制的一个很好说明</li>
<li><a href="https://zhuanlan.zhihu.com/p/32052223">Raft算法详解</a></li>
</ul>
</div></div><div class="post-main post-comment"><div id="giscus_thread"></div><script src="https://giscus.app/client.js" data-repo="lotabout/lotabout.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkyMDU1NTQ0Nw==" data-category="Announcements" data-category-id="DIC_kwDOATmmt84ClmcD" data-mapping="" data-strict="" data-reactions-enabled="0" data-emit-metadata="" data-input-position="" data-theme="" data-lang="zh-CN" data-loading="" crossorigin="" async>
</script></div></article><link rel="stylesheet" type="text/css" href="/css/third-party/font-awesome/4.5.0/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/fancybox/2.1.5/jquery.fancybox.css"><script src="/js/third-party/jquery/2.0.3/jquery.min.js"></script><script src="/js/third-party/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=#{theme.google_analytics}"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-39956831-2');</script></body></html>