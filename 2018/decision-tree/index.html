<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="&lt;p&gt;通过训练，我们可以从样本中学习到决策树，作为预测模型来预测其它样本。两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们说要训练/学习，训练/学习什么？&lt;/li&gt;
&lt;li&gt;为什么决策树可以用来预测？或者说它的泛化能力的来源是哪？&lt;/li&gt;
&lt;/ol&gt;"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>决策树 (decision tree) | 三点水</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/books">Books</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-noise-link" href="/tags/Statistics/" rel="tag">Statistics</a><a class="post-tag-noise-link" href="/tags/decision-tree/" rel="tag">decision tree</a></div><div class="post-time">2018-03-02</div></div></div><div class="container post-header"><h1>决策树 (decision tree)</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">什么是决策树？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-%E8%AE%AD%E7%BB%83%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">训练，训练什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">3.</span> <span class="toc-text">泛化能力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E7%82%B9%E7%9A%84%E5%90%88%E5%B9%B6%E6%98%AF%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%A0%B9%E6%9C%AC"><span class="toc-number">3.1.</span> <span class="toc-text">节点的合并是泛化能力的根本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B1%BB%E5%88%AB%E7%9A%84%E5%88%87%E5%88%86%E4%B9%9F%E8%83%BD%E6%8F%90%E4%BE%9B%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">3.2.</span> <span class="toc-text">类别的切分也能提供泛化能力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%AA%E5%BF%83%E6%8C%87%E6%A0%87"><span class="toc-number">4.</span> <span class="toc-text">贪心指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E4%B8%8D%E7%BA%AF%E5%BA%A6-gini-impurity"><span class="toc-number">4.1.</span> <span class="toc-text">基尼不纯度 Gini impurity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5%E5%A2%9E%E7%9B%8A-information-gain"><span class="toc-number">4.2.</span> <span class="toc-text">信息熵增益 Information Gain</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.</span> <span class="toc-text">决策树的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%86%B5%E5%A2%9E%E7%9B%8A"><span class="toc-number">5.1.</span> <span class="toc-text">熵增益</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol></details></div><div class="container post-content"><p>通过训练，我们可以从样本中学习到决策树，作为预测模型来预测其它样本。两个问题：</p>
<ol>
<li>我们说要训练/学习，训练/学习什么？</li>
<li>为什么决策树可以用来预测？或者说它的泛化能力的来源是哪？</li>
</ol>
<span id="more"></span>
<h2 id="什么是决策树？"><a class="header-anchor" href="#什么是决策树？"></a>什么是决策树？</h2>
<p>一棵“树”，目的和作用是“决策”。一般来说，每个节点上都保存了一个切分，输入数据通过切分继续访问子节点，直到叶子节点，就找到了目标，或者说“做出了决策”。这里我们举个喜闻乐见的例子吧。</p>
<p>现在有人给你介绍对象，你打听到对方的特点：白不白，富不富，美不美，然后决定去不去相亲。根据以往经验，我们给出所有可能性：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 白   | 富   | 美   | 去   |</span><br><span class="line">| 白   | 富   | 不美 | 去   |</span><br><span class="line">| 白   | 不富 | 美   | 犹豫 |</span><br><span class="line">| 白   | 不富 | 不美 | 犹豫 |</span><br><span class="line">| 不白 | 富   | 美   | 去   |</span><br><span class="line">| 不白 | 富   | 不美 | 去   |</span><br><span class="line">| 不白 | 不富 | 美   | 犹豫 |</span><br><span class="line">| 不白 | 不富 | 不美 | 不去 |</span><br></pre></td></tr></table></figure></div>
<p>那么有人给我们介绍新的对象的时候，我们就要一个个特点去判断，于是这种判断的过程就可以画成一棵树，例如根据特点依次判断：</p>
<img src="/2018/decision-tree/decision-tree-abc.1.png" class="" title="Decision Tree ABC Full">
<p>这就是决策树，每一层我们都提出一个问题，根据问题的回答来走向不同的子树，最终到达叶子节点时，做出决策（去还是不去）。可以看到，决策树没有任何特别的地方。</p>
<p>当然，如果我们先考虑富不富，再考虑白不白，则得到的树又不相同：</p>
<img src="/2018/decision-tree/decision-tree-bac.1.png" class="" title="Decision Tree BAC Full">
<p>所以，决策树其实就是根据已知的经验来构建一棵树。可以认为是根据数据的某个维度进行切分，不断重复这个过程。当然，如果切分的顺序不同，会得到不同的树。</p>
<p>既然如此，按不同顺序切分得到的决策树又有什么不同呢？</p>
<h2 id="训练-训练什么？"><a class="header-anchor" href="#训练-训练什么？"></a>训练，训练什么？</h2>
<p>如果仔细观察，我们发现决策树中有一些叶子节点是可以合并的，合并之后，到达某个节点时就不需要进行额外的决策，例如切分顺序“白，富，美”得到的决策树合并后如下：</p>
<img src="/2018/decision-tree/decision-tree-abc.2.png" class="" title="Decision Tree ABC Abbreviate">
<p>我们先记着，合并后的树有 5 个叶子节点。</p>
<p>而“富，白，美”的决策树合并后变成：</p>
<img src="/2018/decision-tree/decision-tree-bac.2.png" class="" title="Decision Tree BAC Abbreviate">
<p>可以看到上面这棵树则只有 4 个叶子节点，少于“白，富，美”的 5 个节点。</p>
<p>这就是决策树间最大的区别，不同决策树合并后得到树叶子节点的个数是不同的，后面我们会看到，叶子节点越少，往往决策树的泛化能力越高，所以可以认为训练决策树的一个目标是 <strong>减少决策树的叶子节点</strong> 。这个任务其实是很困难的，考虑数据有 <code>n</code> 维，那么切分的顺序的可能性就是<code>n!</code>。因此实际中一般并不是求全局最优，而是采用贪心算法求局部最优。</p>
<p>另外，节点在什么时候才能合并呢？一般需要叶子节点的标签/决策相同。也因此，后面提到贪心的指标时，往往指标的目的就是选择某一个维度，使得划分后的子集合更 <strong>有序</strong>。</p>
<p>（当然上面的说法不准确，决策树就是一棵树，建成什么样子其实全凭心情/需求。在搜索过程中其实并找不到官方的优化目标，上面的结论是博主自己得出的，而它能帮助我们理解下一个问题：决策树为什么有泛化能力？）</p>
<h2 id="泛化能力"><a class="header-anchor" href="#泛化能力"></a>泛化能力</h2>
<p>细心的读者会发现，决策树好像根本没什么用？</p>
<p>在上面的例子里，我们只需要记住切分的顺序，例如“富，白美”，然后在原数据中一个个匹配就行了，树的结构虽然方便理解，但它也没有存在的必要。而这个疑问的一个引申，既然我们能通过查表来做决策，但之前又说决策树可以用来做预测，那么决策树的泛化能力（即“预测”能力）来自哪里呢？</p>
<h3 id="节点的合并是泛化能力的根本"><a class="header-anchor" href="#节点的合并是泛化能力的根本"></a>节点的合并是泛化能力的根本</h3>
<p>上面的例子中我们有三个维度，每个维度有两种可能，并且我们的经验已经覆盖了所有的
8 种情况。但实际生活中我们的样本不可能覆盖所有的可能性，因此在我们合并节点的过程中就悄悄地覆盖了一些未知的数据。例如我们只遇到过 4 种情况（这里的决策和上面的例子不同）：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 白   | 富   | 美   | 去   |</span><br><span class="line">| 白   | 不富 | 美   | 犹豫 |</span><br><span class="line">| 不白 | 不富 | 美   | 犹豫 |</span><br><span class="line">| 不白 | 不富 | 不美 | 不去 |</span><br></pre></td></tr></table></figure></div>
<p>在此基础上构建决策树（顺序为富，白，美）：</p>
<img src="/2018/decision-tree/decision-tree-missing.1.png" class="" title="Decision Tree Missing Full">
<p>于是在合并的过程中，我们把没有见过数据都忽略，于是合并后的树为：</p>
<img src="/2018/decision-tree/decision-tree-missing.2.png" class="" title="Decision Tree Missing Abbreviate">
<p>对于这棵决策树，如果遇到新的数据 <code>白，富，不美</code> 我们也可以推测值得 <code>去</code>，或者对于 <code>白，不富，不美</code> 我们就会 <code>犹豫</code>。</p>
<p>当然，我们的示例自下向上合并，是为了方便展示，实际构建决策树时，如果某个分支只有一个样本，会直接停止展开。如，在“富”的分支上，我们只见到一个样本，结果是“去”，因此会停止继续向下展开。这和算法中的剪枝是相同的想法。</p>
<h3 id="类别的切分也能提供泛化能力"><a class="header-anchor" href="#类别的切分也能提供泛化能力"></a>类别的切分也能提供泛化能力</h3>
<p>上面的例子里我们的数据都是“类别型”，即一个维度/特征的取值是离散的，例如“富”，只能取两个值“富”和“不富”。但实际生活中，一个维度的取值可以是连续的，例如人的身高，体重，工资等。</p>
<p>那么，当决策树的一个节点需要切分时，我们不可能穷举所有的可能，因此需要做一定的取舍。常见的作法是对数据做一个“二切分”。例如我们知道三个人的身高： <code>153</code>,
<code>164</code>, <code>182</code>，我们切分成 <code>&lt;= 164</code> 和 <code>&gt; 164</code> 两类（也可以用其它的切分方法）。</p>
<p>这种 <strong>模糊的切分</strong> 也提供了泛化能力。例如一个新的数据，身高 <code>175</code>，我们自然就能归到 <code>&gt; 164</code> 的切分中，即使之前根本没见过这个数据。</p>
<h2 id="贪心指标"><a class="header-anchor" href="#贪心指标"></a>贪心指标</h2>
<p>一般我们是用贪心算法来构建决策树，这就引申出了一些常用的指标，帮助我们决定在每次切分时，选择哪个维度进行切分；遇到数值类型需要做二切分时，具体用哪个数值。下面我们介绍两个常用的指标：</p>
<h3 id="基尼不纯度-gini-impurity"><a class="header-anchor" href="#基尼不纯度-gini-impurity"></a>基尼不纯度 Gini impurity</h3>
<p>一个集合有 $J$ 个类别，我们记 $i \in { 1, 2, …, J }$，且 $p_i$ 表示该集合中标记为类别 $i$ 的元素所占的比例，则 <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">基尼不纯度
</a> 定义为：</p>
<p>$$
I_{G}( p ) = \sum_{i=1}^J p_i \sum_{k\neq i} p_k
= \sum_{i=1}^{J} p_i (1-p_i)
= \sum_{i=1}^{J} (p_i - {p_i}^2)
= \sum_{i=1}^J p_i - \sum_{i=1}^{J} {p_i}^2
= 1 - \sum^{J}_{i=1} {p_i}^{2}
$$</p>
<p>想象我们有一堆乒乓球，和一堆标签，为每个球上贴一个标签，这组成了我们的原始样本。现在，我们再买和之前一样的一堆标签，为每个球上再贴一个标签。那么现在球上有两个标签，它们可能一样，也可能不一样。基尼不纯度指的就是贴了不同标签的球的占比。一个很直观的结论是，如果集合里的标签都一样，那么基尼不纯度就为 <code>0</code>。</p>
<p>所以在数学上，我们可以先考虑标签 $i$，一个球上第一个标签贴为 $j$ 的概率记为
$p_i$，那么贴第二个标签时，要求贴的是非 $i$ 的标签，因此概率是 $\sum_{k \ne
i}{p_k} = 1 - p_i$。那么贴了不同标签的球所占的比例就是 $\sum_{i=1}^J p_i
\sum_{k\neq i} p_k$。</p>
<p>上面提到，决定用哪个维度进行切分时，一个标准是使切分后的子集更 <strong>有序</strong>，这里也意味着基尼不纯度更小。于是我们选择某一个维度进行切分，求得所有子集的基尼不纯度之和。总有一个维度使得这个和取到最小，对应的维度就是当前最佳的切分维度。当然，维度确定后，对于数值型的维度，其实还要确认具体的切分点，也可以用基尼不纯度来作为切分的依据。</p>
<h3 id="信息熵增益-information-gain"><a class="header-anchor" href="#信息熵增益-information-gain"></a>信息熵增益 Information Gain</h3>
<p>首先要了解的是 <a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)">信息熵</a> 在有限样本时定义为：</p>
<p>$$
H(X) = \sum_{i}{P(x_i)I(x_i)} = -\sum_i{P(x_i)\log_2{P(x_i)}}
$$</p>
<p>$-\log_2{P(x_i)}$ 的大意是一个事件 $x_i$ 如果出现的概率越小，那么当它发生时我们就越吃惊，代表的就是一个事件“吃惊程度”。而熵就是所有事件的“吃惊程度”的期望值。一般地，如果一个集合的熵越大，则集合越无序；熵越小，则集合越有序。换句话说，如果熵越大，说明我们越容易吃惊，说明集合无序，我们很难预测下一个出现的是什么，相反，熵越小，说明我们越容易猜测集合里有什么，说明集合越有序。</p>
<p>而在决策树的切分里，事件 $x_i$ 可以认为是在样本中出现某个标签/决策。于是
$P(x_i)$可以用所有样本中某个标签出现的频率来代替。</p>
<p>但我们求熵是为了决定采用哪一个维度进行切分，因此有一个新的概念 <a href="https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5">条件熵</a>：</p>
<p>$$ H(X|Y) = \sum_{y \in Y}{p(y) H(X|Y=y)} $$</p>
<p>这里我们认为 $Y$ 就是用某个维度进行切分，那么 $y$ 就是切成的某个子集合于是
$H(X|Y=y)$ 就是这个子集的熵。因此可以认为就条件熵是每个子集合的熵的一个加权平均/期望。最后，如何判断一个维度更优秀呢？我们采用信息熵增益：</p>
<p>$$Gain(Y) = H(X) - H(X|Y)$$</p>
<p>即切分后，<code>Gain</code> 最高的那个维度，我们优先用它来切分子集。</p>
<h2 id="决策树的实现"><a class="header-anchor" href="#决策树的实现"></a>决策树的实现</h2>
<p>这里用 python 来实现一下基本的决策树（非数值型），再用上面的例子实验实验。完整代码请见
<a href="https://gist.github.com/lotabout/ae2401b091bd7faf4ae6230666f53568/2844bb083d976a21a56f4acf0080b2be35ee28b9">gist</a>
。</p>
<p>首先决定输入的结构：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [[<span class="string">&#x27;白&#x27;</span>,   <span class="string">&#x27;富&#x27;</span>,   <span class="string">&#x27;美&#x27;</span>,   <span class="string">&#x27;去&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;白&#x27;</span>,   <span class="string">&#x27;富&#x27;</span>,   <span class="string">&#x27;不美&#x27;</span>, <span class="string">&#x27;去&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;白&#x27;</span>,   <span class="string">&#x27;不富&#x27;</span>, <span class="string">&#x27;美&#x27;</span>,   <span class="string">&#x27;犹豫&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;白&#x27;</span>,   <span class="string">&#x27;不富&#x27;</span>, <span class="string">&#x27;不美&#x27;</span>, <span class="string">&#x27;犹豫&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;不白&#x27;</span>, <span class="string">&#x27;富&#x27;</span>,   <span class="string">&#x27;美&#x27;</span>,   <span class="string">&#x27;去&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;不白&#x27;</span>, <span class="string">&#x27;富&#x27;</span>,   <span class="string">&#x27;不美&#x27;</span>, <span class="string">&#x27;去&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;不白&#x27;</span>, <span class="string">&#x27;不富&#x27;</span>, <span class="string">&#x27;美&#x27;</span>,   <span class="string">&#x27;犹豫&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;不白&#x27;</span>, <span class="string">&#x27;不富&#x27;</span>, <span class="string">&#x27;不美&#x27;</span>, <span class="string">&#x27;不去&#x27;</span>]]</span><br></pre></td></tr></table></figure></div>
<p>数据是一个 List，每一个元素也是一个 List，代表样本的多个维度，最后一维存放标签。</p>
<p>下面先实现一个切分的函数，作用是将一系列样本，根据某个维度，切分到不同的集合。</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_split_samples</span>(<span class="params">self, samples, feature</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Split samples into subsets, according to the feature</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :samples: List[List[val]]</span></span><br><span class="line"><span class="string">    :feature: Int</span></span><br><span class="line"><span class="string">    :returns: &#123;val: List[data]&#125; a dict contains the data of subsets</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ret = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> samples:</span><br><span class="line">        val = sample[feature]</span><br><span class="line">        ret.setdefault(val, [])</span><br><span class="line">        ret[val].append(sample)</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></div>
<p>有了切分的函数我们就能创建决策树了，下面这个函数是递归调用，给定一些数据，如果
<code>_stop_now</code> 判断已经不需要继续切分了，则返回这些数据的标签（一般来说这些数据的标签会相同），否则我们调用 <code>_get_feature</code> 来决定用哪个维度进行切分，并对每个子集合调用递归调用 <code>_split</code> 创建节点。</p>
<p>树的节点我们用 dict 表示，例如 <code>&#123;'白': ..., '不白', ...&#125;</code>。</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_split</span>(<span class="params">self, data, level=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;recursively split the data for node</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :data: List[data]</span></span><br><span class="line"><span class="string">    :returns: label if should stop, else a node of the tree</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>._stop_now(data):</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>][-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split the data</span></span><br><span class="line">    feature = <span class="variable language_">self</span>._get_feature(data, level)</span><br><span class="line">    subsets = <span class="variable language_">self</span>._split_samples(data, feature)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;key: <span class="variable language_">self</span>._split(subset, level+<span class="number">1</span>) <span class="keyword">for</span> key, subset <span class="keyword">in</span> subsets.items()&#125;</span><br></pre></td></tr></table></figure></div>
<p>接下来，我们只需要实现 <code>_stop_now</code> 和 <code>_get_feature</code> 就可以了。对于
<code>_stop_now</code>，我们认为如果所有样本都是同一个标签就可以停止：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_stop_now</span>(<span class="params">self, data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;check if we need to stop now</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :data: List[data]</span></span><br><span class="line"><span class="string">    :returns: Boolean</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    labels = [d[-<span class="number">1</span>] <span class="keyword">for</span> d <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="built_in">set</span>(labels)) &lt;= <span class="number">1</span></span><br></pre></td></tr></table></figure></div>
<p>而 <code>_get_feature</code>，我们按输入的维度顺序切分，因此实现是：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_feature</span>(<span class="params">self, data, level</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Decide which feature to be used to split data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :data: List[data]</span></span><br><span class="line"><span class="string">    :level: Int the level of the tree</span></span><br><span class="line"><span class="string">    :returns: Int the dimension of the data to be used for split data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> level</span><br></pre></td></tr></table></figure></div>
<p>最后把上面这些代码放到一个类里：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecisionTree</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Learn a decision tree from data and label</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :data: List[List[val]], a list contains M sample, each sample is represented by a List</span></span><br><span class="line"><span class="string">               The last column of the sample is the label</span></span><br><span class="line"><span class="string">        :returns: The root of a decision tree</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(DecisionTree, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.root = <span class="variable language_">self</span>._split(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># rest of the methods</span></span><br><span class="line"></span><br><span class="line">tree = DecisionTree(data)</span><br><span class="line"><span class="built_in">print</span>(tree.root)</span><br></pre></td></tr></table></figure></div>
<p>得到的结果是：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    &#x27;白&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &#x27;富&#x27;<span class="punctuation">:</span> &#x27;去&#x27;<span class="punctuation">,</span></span><br><span class="line">        &#x27;不富&#x27;<span class="punctuation">:</span> &#x27;犹豫&#x27;</span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    &#x27;不白&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &#x27;富&#x27;<span class="punctuation">:</span> &#x27;去&#x27;<span class="punctuation">,</span></span><br><span class="line">        &#x27;不富&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            &#x27;美&#x27;<span class="punctuation">:</span> &#x27;犹豫&#x27;<span class="punctuation">,</span></span><br><span class="line">            &#x27;不美&#x27;<span class="punctuation">:</span> &#x27;不去&#x27;</span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></div>
<p>可以看到，和我们前面手工合并的结果是一样的。</p>
<h3 id="熵增益"><a class="header-anchor" href="#熵增益"></a>熵增益</h3>
<p>下面我们实现信息熵增益指标，首先是熵的计算</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_entropy</span>(<span class="params">self, dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;calculate the entropy of a dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :dataset: List[data], each data is List[val], last column is label</span></span><br><span class="line"><span class="string">    :returns: Float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    counts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">        label = data[-<span class="number">1</span>]</span><br><span class="line">        counts.setdefault(label, <span class="number">0</span>)</span><br><span class="line">        counts[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    total_num = <span class="built_in">len</span>(dataset)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>([-count/total_num * math.log2(count/total_num) <span class="keyword">for</span> count <span class="keyword">in</span> counts.values()])</span><br></pre></td></tr></table></figure></div>
<p>然后是条件熵的计算：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_conditional_entropy</span>(<span class="params">self, dataset, feature</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;calculate the conditional entropy of dataset on feature</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :dataset: List[data]</span></span><br><span class="line"><span class="string">    :feature: Int</span></span><br><span class="line"><span class="string">    :returns: Float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    subsets = <span class="variable language_">self</span>._split_samples(dataset, feature)</span><br><span class="line">    total_num = <span class="built_in">len</span>(subsets)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>([<span class="built_in">len</span>(subset)/total_num * <span class="variable language_">self</span>._entropy(subset) <span class="keyword">for</span> subset <span class="keyword">in</span> subsets.values()])</span><br></pre></td></tr></table></figure></div>
<p>最后，替换之前的 <code>_get_feature</code>，也就是在决定用什么维度进行切分时，我们选择熵增益最大的维度：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_feature</span>(<span class="params">self, data, level</span>):</span><br><span class="line">    dimensions = <span class="built_in">len</span>(data[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    entropy = <span class="variable language_">self</span>._entropy(data)</span><br><span class="line"></span><br><span class="line">    gains = [entropy - <span class="variable language_">self</span>._conditional_entropy(data, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dimensions)]</span><br><span class="line">    <span class="keyword">return</span> gains.index(<span class="built_in">max</span>(gains))</span><br></pre></td></tr></table></figure></div>
<p>我们再用这个策略去“训练”前面的数据，得到的结果为：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    &#x27;富&#x27;<span class="punctuation">:</span> &#x27;去&#x27;<span class="punctuation">,</span></span><br><span class="line">    &#x27;不富&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &#x27;白&#x27;<span class="punctuation">:</span> &#x27;犹豫&#x27;<span class="punctuation">,</span></span><br><span class="line">        &#x27;不白&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            &#x27;美&#x27;<span class="punctuation">:</span> &#x27;犹豫&#x27;<span class="punctuation">,</span></span><br><span class="line">            &#x27;不美&#x27;<span class="punctuation">:</span> &#x27;不去&#x27;</span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></div>
<p>可以看到，结果对应了“富，白，美”的切分顺序。而之前我们也知道，这个顺序有 4 个叶子节点，而默认切分 “白，富，美” 有 5 个叶子节点。也证明这样的优化目标是有效的。</p>
<h2 id="小结"><a class="header-anchor" href="#小结"></a>小结</h2>
<p>“训练”决策树是为了减少决策树最后的叶子节点，由于训练全局最优很困难，因此人们用一些局部的贪心策略进行训练，例如上文介绍的信息熵增益。</p>
<p>决策树的泛化能力主要来源于叶节点的合并。因此，如果决策树“过拟合”，其实意味着合并的节点不够多。</p>
<p>最后，本文代码的完整版请见 <a href="https://gist.github.com/lotabout/ae2401b091bd7faf4ae6230666f53568">Gist: decision tree</a></p>
<h2 id="参考"><a class="header-anchor" href="#参考"></a>参考</h2>
<ul>
<li><a href="http://www.csuldw.com/2015/05/08/2015-05-08-decision%20tree/">http://www.csuldw.com/2015/05/08/2015-05-08-decision tree/</a> 对优化指标有很好的讲解。</li>
<li><a href="http://blog.csdn.net/xbinworld/article/details/44660339">http://blog.csdn.net/xbinworld/article/details/44660339</a> 讲解了一些实现上的注意点，如过拟合，剪枝。</li>
<li><a href="https://www.geeksforgeeks.org/decision-tree-introduction-example/">https://www.geeksforgeeks.org/decision-tree-introduction-example/</a> 包含了数值型数据的一些实现</li>
</ul>
</div></div><script type="text/x-mathjax-config">
   MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    }); 
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="post-main post-comment"><div id="giscus_thread"></div><script src="https://giscus.app/client.js" data-repo="lotabout/lotabout.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkyMDU1NTQ0Nw==" data-category="Announcements" data-category-id="DIC_kwDOATmmt84ClmcD" data-mapping="" data-strict="" data-reactions-enabled="0" data-emit-metadata="" data-input-position="" data-theme="" data-lang="zh-CN" data-loading="" crossorigin="" async>
</script></div></article><link rel="stylesheet" type="text/css" href="/css/third-party/font-awesome/4.5.0/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/fancybox/2.1.5/jquery.fancybox.css"><script src="/js/third-party/jquery/2.0.3/jquery.min.js"></script><script src="/js/third-party/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=#{theme.google_analytics}"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-39956831-2');</script></body></html>