<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="前情提要：在&lt;a href=&quot;/2023/Auto-Differentiation-Part-1-Algorithm/&quot; title=&quot;自动微分（Automatic Differentiation）：算法篇&quot;&gt;算法篇&lt;/a&gt;中，我们介绍了深度学习领域基本都是使用自动微分(Automatic Differentiation)来计算偏导的。本篇中我们要尝试自己做一个实现。"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>自动微分（Automatic Differentiation）：实现篇 | 三点水</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/books">Books</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-noise-link" href="/tags/Automatic-Differentiation/" rel="tag">Automatic Differentiation</a><a class="post-tag-noise-link" href="/tags/Neural-Network/" rel="tag">Neural Network</a></div><div class="post-time">2023-04-16</div></div></div><div class="container post-header"><h1>自动微分（Automatic Differentiation）：实现篇</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87"><span class="toc-number">1.</span> <span class="toc-text">目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF"><span class="toc-number">2.</span> <span class="toc-text">整体思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">框架实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor"><span class="toc-number">3.1.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#operator"><span class="toc-number">3.2.</span> <span class="toc-text">Operator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E9%81%8D%E5%8E%86"><span class="toc-number">3.3.</span> <span class="toc-text">计算图遍历</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E7%AE%97%E5%AD%90"><span class="toc-number">4.</span> <span class="toc-text">具体算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E5%8A%A0%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">元素加法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E4%B9%98%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">元素乘法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sin"><span class="toc-number">4.3.</span> <span class="toc-text">sin</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E4%B8%8E%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.</span> <span class="toc-text">向量与实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B7%E4%BE%8B%E5%AE%9E%E8%B7%91"><span class="toc-number">5.1.</span> <span class="toc-text">样例实跑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E5%88%B0%E5%90%91%E9%87%8F"><span class="toc-number">5.2.</span> <span class="toc-text">扩展到向量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol></details></div><div class="container post-content"><p>前情提要：在<a href="/2023/Auto-Differentiation-Part-1-Algorithm/" title="自动微分（Automatic Differentiation）：算法篇">算法篇</a>中，我们介绍了深度学习领域基本都是使用自动微分(Automatic Differentiation)来计算偏导的。本篇中我们要尝试自己做一个实现。</p>
<h2 id="目标"><a class="header-anchor" href="#目标"></a>目标</h2>
<p>如果有函数 $f(x_1, \cdots, x_n)$，我们要使用链式法则计算函数 $f$ 对所有输入
$x_i$ 的偏导。我们记中间函数为 $v$，记 $\bar{v_i} = \frac{\partial f}{\partial
v_i}$，则最核心的计算公式为：</p>
<p>$$
\bar{v_i} = \frac{\partial f}{\partial v_i}
= \sum_{j \in next(i)}{\overline{v_{i\to j}}}
= \sum_{j \in next(i)}{\frac{\partial f}{\partial v_{j}} \frac{\partial v_{j}}{\partial v_i}
= \sum_{j \in next(i)}{\overline{v_j} \frac{\partial v_{j}}{\partial v_i}}}
$$</p>
<p>大家可以配合算法篇的图来理解：</p>
<img src="/2023/Auto-Differentiation-Part-2-Implementation/2023-04-AD-example-computation-graph.svg" class="">
<h2 id="整体思路"><a class="header-anchor" href="#整体思路"></a>整体思路</h2>
<p>首先需要允许用户构建计算图，很自然地关心 3 个部分：</p>
<ol>
<li>节点。计算图中的节点代表了计算，如 <code>sin</code> 这样的函数，我们把它叫作算子
(operator)。在 AD 的场景下，算子既要关心前向计算，也需要关心后向求导</li>
<li>边。需要有办法找到算子的上下游，在 AD 中我们使用邻接表来表示<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></li>
<li>边上流转的数据。边上流转的有正向的计算数据，逆向的偏导数据，我们会统一用张量(Tensor)来表示</li>
</ol>
<p>要注意的是为了符合用户的使用习惯，我们并不是要求用户直接给出一个“节点” List，再给出一个“边” List。计算图是隐式构建的。因此实际上是 <code>数据 --(来源)--&gt; 节点 --(输入)--&gt; 数据</code> 这样的引用关系。</p>
<p>计算图构建好之后，需要有遍历引擎，按拓扑排序顺序，正向地、逆向地遍历所有节点，正向计算输出，逆向计算偏导。这里的执行引擎其实有很多可以优化的空间，比如多线程计算，多节点合并计算等，但本文里就是简单地走流程。</p>
<p>最终希望怎么使用呢？</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = Tensor(np.array([<span class="number">0.5</span>]), requires_grad=<span class="literal">True</span>)</span><br><span class="line">x2 = Tensor(np.array([<span class="number">0.5</span>]), requires_grad=<span class="literal">True</span>)</span><br><span class="line">v3 = sin(x1)</span><br><span class="line">v4 = mul(x1, x2)</span><br><span class="line">v5 = add(v3, v4)</span><br><span class="line">grad = np.array([<span class="number">1</span>])</span><br><span class="line">v5.backward(grad)</span><br><span class="line"><span class="built_in">print</span>(x1.grad) <span class="comment"># expected to be 1.37758</span></span><br><span class="line"><span class="built_in">print</span>(x2.grad) <span class="comment"># expected to be 0.5</span></span><br></pre></td></tr></table></figure></div>
<h2 id="框架实现"><a class="header-anchor" href="#框架实现"></a>框架实现</h2>
<h3 id="tensor"><a class="header-anchor" href="#tensor"></a>Tensor</h3>
<p>我们用张量 <code>Tensor</code> 来定义数据部分。代码如下：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Tensor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;tensor&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ndarray: NDArray, requires_grad=<span class="literal">False</span>, grad_fn=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Tensor, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.ndarray = ndarray            <span class="comment"># ①</span></span><br><span class="line">        <span class="variable language_">self</span>.requires_grad = requires_grad  <span class="comment"># ②</span></span><br><span class="line">        <span class="variable language_">self</span>.grad_fn = grad_fn            <span class="comment"># ③</span></span><br><span class="line">        <span class="variable language_">self</span>.grad = <span class="literal">None</span>                  <span class="comment"># ④</span></span><br><span class="line">        <span class="variable language_">self</span>._grad_accmulator = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_leaf</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.requires_grad <span class="keyword">and</span> <span class="variable language_">self</span>.grad_fn <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, output_grad</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.grad_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> <span class="string">&quot;backward could not be called if grad_fn is None&quot;</span></span><br><span class="line">        execute_graph(<span class="variable language_">self</span>.grad_fn, output_grad)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        grad_info = <span class="string">f&#x27; grad_fn=<span class="subst">&#123;self.grad_fn&#125;</span>&#x27;</span> <span class="keyword">if</span> <span class="variable language_">self</span>.grad_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&#x27;tensor(<span class="subst">&#123;self.ndarray&#125;</span><span class="subst">&#123;grad_info&#125;</span>)&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.__str__()</span><br></pre></td></tr></table></figure></div>
<p>① 中使用 <code>numpy.ndarray</code> 保存前向数据，直接使用 numpy 来减少复杂度，毕竟我们只关心 AD 部分</p>
<p>③ 的 <code>grad_fn</code> 可以理解成保存的是 <code>Tensor</code> 的来源算子。实际上当 Tensor 生成时，对应的数据就计算完成了，记录它的来源也没有意义，但由于后续还要反向计算偏导，才需要记录来源来反查。因此只有在 ② <code>requires_grad = True</code> 时才有记录的必要。</p>
<p>④ 的 <code>grad</code> 就是偏导的结果，即 $\bar{v_i}$ 的值。</p>
<h3 id="operator"><a class="header-anchor" href="#operator"></a>Operator</h3>
<p>首先算子既需要管前向计算，也需要关心后向求导，于是框架性的定义如下：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意 Operator 里计算的都是 Tensor 内部的数据，即 NDArray</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Operator</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Operator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.next_ops = [] <span class="comment"># ①</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, *args: <span class="type">Tuple</span>[NDArray]</span>) -&gt; NDArray:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Should be override by subclass&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, output_grad: <span class="type">Tuple</span>[NDArray]</span>) -&gt; <span class="type">Union</span>[NDArray, <span class="type">Tuple</span>[NDArray]]:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Should be override by subclass&quot;</span>)</span><br></pre></td></tr></table></figure></div>
<p><code>forward</code> 代表前向计算，可以有多个输入。<code>backward</code> 则相反，给定输出的偏导，需要为每个输入输出一个偏导。即如果 $op = f(x, y)$，则 <code>forward</code> 输出的是
$f(x, y)$ 的值，而 <code>backward</code> 输出为 $[\frac{\partial op}{\partial x}, \frac{\partial op}{\partial y}]$</p>
<p>但仅有两个计算方法是不够的，在 <code>forward</code> 计算时，算子还需要维护“边”的信息，在后向计算偏导时使用。①中的 <code>next_ops</code> 就是用来计算边的信息的，例如样例代码中，执行完 <code>v5 = add(v3, v4)</code> 后，内部信息如下图：</p>
<img src="/2023/Auto-Differentiation-Part-2-Implementation/2023-04-AD-Op-Graph.svg" class="">
<p>但我们不希望建图的操作在每个算子中都实现一遍，因此我们在父类上实现 <code>__call__</code>
函数，在使用时用户不应该直接调用 <code>forward</code> 函数，而应该直接调用 <code>__call__</code> 函数，实现如下：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, *args: <span class="type">Tuple</span>[Tensor]</span>) -&gt; Tensor:</span><br><span class="line">    grad_fn = <span class="literal">None</span></span><br><span class="line">    requires_grad = <span class="built_in">any</span>((t.requires_grad <span class="keyword">for</span> t <span class="keyword">in</span> args)) <span class="comment"># ①</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> requires_grad:</span><br><span class="line">        <span class="comment"># add edges</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> args:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">input</span>.is_leaf(): <span class="comment"># ②</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">input</span>._grad_accmulator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="built_in">input</span>._grad_accmulator = OpAccumulate(<span class="built_in">input</span>)</span><br><span class="line">                <span class="variable language_">self</span>.next_ops.append(<span class="built_in">input</span>._grad_accmulator)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.next_ops.append(<span class="built_in">input</span>.grad_fn) <span class="comment"># ③</span></span><br><span class="line">        grad_fn = <span class="variable language_">self</span></span><br><span class="line"></span><br><span class="line">    inputs = [t.ndarray <span class="keyword">for</span> t <span class="keyword">in</span> args]</span><br><span class="line">    output = <span class="variable language_">self</span>.forward(*inputs) <span class="comment"># ④</span></span><br><span class="line">    <span class="keyword">return</span> Tensor(output, requires_grad=requires_grad, grad_fn=grad_fn) <span class="comment"># ⑤</span></span><br></pre></td></tr></table></figure></div>
<p>其中 ① 会将输入 Tensor 的 <code>requires_grad</code> 值传染给输出，算子任意输入 Tensor 中，只要有一个需要算梯度，则输出的 Tensor 也需要计算梯度。另外④中可以看出
<code>__call__</code> 就是 <code>forward</code> 方法的包装。注意到 <code>forward</code> 的输出是 ndarray，而因为算子输出也需要是 Tensor，因此在 ⑤ 中做了封装。</p>
<p>在构造计算图时，会将 <code>input.grad_fn</code> 指向的算子，加入 <code>next_ops</code> 中，如 ③ 所示。只有②的例外，如果输入本身就是叶子节点，则它的 <code>grad_fn</code> 没有指向任何节点，因此这里构造了一个特殊的 <code>OpAccumulate</code> 算子来累加并设置梯度，如下所示：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OpAccumulate</span>(<span class="title class_ inherited__">Operator</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tensor</span>):</span><br><span class="line">        <span class="built_in">super</span>(OpAccumulate, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.tensor = tensor</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grad</span>):</span><br><span class="line">        <span class="variable language_">self</span>.tensor.grad = Tensor(grad)</span><br><span class="line">        <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure></div>
<h3 id="计算图遍历"><a class="header-anchor" href="#计算图遍历"></a>计算图遍历</h3>
<p>计算图是一个有向无环图（简称 DAG），DAG 遍历的重点是需要按拓扑排序遍历，在一个算子的所有输入都被满足时才能执行该算子的 <code>backward</code> 方法。于是我们先搞个辅助函数，按拓扑的顺序，统计每个算子依赖的输入个数。</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_dependencies</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="comment"># deps: &#123;op: num&#125;</span></span><br><span class="line">    deps = &#123;&#125;</span><br><span class="line">    q = deque()</span><br><span class="line">    traversed = &#123;root&#125;</span><br><span class="line">    q.append(root)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(q) != <span class="number">0</span>:</span><br><span class="line">        cur = q.pop()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(cur.next_ops) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">next</span> <span class="keyword">in</span> cur.next_ops:</span><br><span class="line">            deps[<span class="built_in">next</span>] = deps.get(<span class="built_in">next</span>, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">next</span> <span class="keyword">not</span> <span class="keyword">in</span> traversed:</span><br><span class="line">                q.append(<span class="built_in">next</span>)</span><br><span class="line">                traversed.add(<span class="built_in">next</span>)</span><br><span class="line">    <span class="keyword">return</span> deps</span><br></pre></td></tr></table></figure></div>
<p>在样例代码里，最终会以 <code>root = op:+</code> 来调用，因此它会返回类似如下信息（当然
key 会是各个实例化的算子，而不是字符串）：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;op:+&quot;: 1,</span><br><span class="line">  &quot;op:sin&quot;: 1,</span><br><span class="line">  &quot;op:*&quot;: 1,</span><br><span class="line">  &quot;op:acc|x1&quot;: 2,</span><br><span class="line">  &quot;op:acc|x2&quot;: 1,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>接下来我们会遍历整个图：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">execute_graph</span>(<span class="params">root, output_grad</span>):</span><br><span class="line">    deps = compute_dependencies(root)</span><br><span class="line">    inputs = &#123;root: output_grad&#125;  <span class="comment"># ①</span></span><br><span class="line"></span><br><span class="line">    q = deque()</span><br><span class="line">    q.append(root)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(q) != <span class="number">0</span>:</span><br><span class="line">        task = q.pop()</span><br><span class="line">        <span class="built_in">input</span> = inputs[task]</span><br><span class="line">        outputs = task.backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(outputs, collections.abc.<span class="type">Sequence</span>):</span><br><span class="line">            outputs = [outputs]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> next_op, output <span class="keyword">in</span> <span class="built_in">zip</span>(task.next_ops, outputs):</span><br><span class="line">            <span class="keyword">if</span> next_op <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># accumulate the &quot;inputs&quot; for next_op # ②</span></span><br><span class="line">            op_input = inputs.get(next_op, <span class="number">0</span>)</span><br><span class="line">            inputs[next_op] = op_input + output</span><br><span class="line"></span><br><span class="line">            deps[next_op] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> deps[next_op] == <span class="number">0</span>: <span class="comment"># ③</span></span><br><span class="line">                q.append(next_op)</span><br></pre></td></tr></table></figure></div>
<p>这个遍历过程可说的内容也不多，就是将 ready 的算子一个个放进队列 <code>q</code> 中，一个个执行它们的 <code>backward</code> 方法。其中比较关键的是，如果算子 <code>backward</code> 的输入如果有多个，则需要在 ① 中缓存部分输入，并且在 ② 中当新的输入到来需要进行累加，这里对应了开头公式 $\bar{v_i} = \sum_{j \in next(i)}{\overline{v_{i\to j}}}$ 的部分。最后在 ③ 中，要确保目标算子的所有输入都计算完成，才认为目标算子 ready 了。</p>
<p>如此，所有“框架”层面的内容均实现完毕。</p>
<h2 id="具体算子"><a class="header-anchor" href="#具体算子"></a>具体算子</h2>
<p>有了框架还不够，还需要实现算子，而实现算子最关键的是可能需要在 <code>forward</code> 过程中记录输入信息，在 <code>backward</code> 中用来计算偏导。例如文章开头的样例中 $\bar{v_2}
= \bar{v_4} v_1$ 就需要在 <code>forward</code> 时记录 $v_1$ 的值。下面补齐示例中需要的几个算子</p>
<p>另外注意下面的代码中除了实现算子，我们还实现了诸如 <code>add, mul</code> 等函数，方便对
Tensor 构建计算图。</p>
<h3 id="元素加法"><a class="header-anchor" href="#元素加法"></a>元素加法</h3>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OpEWiseAdd</span>(<span class="title class_ inherited__">Operator</span>):</span><br><span class="line">    <span class="comment"># func: y = a + b</span></span><br><span class="line">    <span class="comment"># deri: y&#x27;/a&#x27; = 1</span></span><br><span class="line">    <span class="comment"># deri: y&#x27;/b&#x27; = 1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, a: NDArray, b: NDArray</span>):</span><br><span class="line">        <span class="keyword">return</span> a + b</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grad: NDArray</span>):</span><br><span class="line">        ret = grad, grad</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> OpEWiseAdd()(a, b)</span><br></pre></td></tr></table></figure></div>
<h3 id="元素乘法"><a class="header-anchor" href="#元素乘法"></a>元素乘法</h3>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OpEWiseMul</span>(<span class="title class_ inherited__">Operator</span>):</span><br><span class="line">    <span class="comment"># func: y = a * b</span></span><br><span class="line">    <span class="comment"># deri: y&#x27;/a&#x27; = b</span></span><br><span class="line">    <span class="comment"># deri: y&#x27;/b&#x27; = a</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, a: NDArray, b: NDArray</span>):</span><br><span class="line">        <span class="variable language_">self</span>.a = a</span><br><span class="line">        <span class="variable language_">self</span>.b = b</span><br><span class="line">        <span class="keyword">return</span> a * b</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grad: NDArray</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.b * grad, <span class="variable language_">self</span>.a * grad</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mul</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> OpEWiseMul()(a, b)</span><br></pre></td></tr></table></figure></div>
<h3 id="sin"><a class="header-anchor" href="#sin"></a>sin</h3>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OpSin</span>(<span class="title class_ inherited__">Operator</span>):</span><br><span class="line">    <span class="comment"># func: y = sin(x)</span></span><br><span class="line">    <span class="comment"># deri: y&#x27;/x&#x27; = cos(x)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: NDArray</span>):</span><br><span class="line">        <span class="variable language_">self</span>.x = x</span><br><span class="line">        <span class="keyword">return</span> np.sin(x)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grad: NDArray</span>):</span><br><span class="line">        ret = np.cos(<span class="variable language_">self</span>.x) * grad</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sin</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> OpSin()(x)</span><br></pre></td></tr></table></figure></div>
<h2 id="向量与实验"><a class="header-anchor" href="#向量与实验"></a>向量与实验</h2>
<h3 id="样例实跑"><a class="header-anchor" href="#样例实跑"></a>样例实跑</h3>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; x1 = Tensor(np.array([0.5]), requires_grad=True)</span><br><span class="line">&gt;&gt;&gt; x2 = Tensor(np.array([0.5]), requires_grad=True)</span><br><span class="line">&gt;&gt;&gt; v3 = sin(x1)</span><br><span class="line">&gt;&gt;&gt; v4 = mul(x1, x2)</span><br><span class="line">&gt;&gt;&gt; v5 = add(v3, v4)</span><br><span class="line">&gt;&gt;&gt; grad = np.array([1])</span><br><span class="line">&gt;&gt;&gt; v5.backward(grad)</span><br><span class="line">&gt;&gt;&gt; print(x1.grad)</span><br><span class="line">tensor([1.37758256])</span><br><span class="line">&gt;&gt;&gt; print(x2.grad)</span><br><span class="line">tensor([0.5])</span><br></pre></td></tr></table></figure></div>
<p>大家可以算算，跟公式算出来是一样的</p>
<h3 id="扩展到向量"><a class="header-anchor" href="#扩展到向量"></a>扩展到向量</h3>
<p>如果 $x_1, x_2$ 是向量呢？这里关系到向量的求导到底要怎么算，但整体来说，咱们实现的框架还是成立的。例如上面例子中的 <code>+, *, sin</code>，如果都只考虑是按元素的操作（不涉及矩阵乘法），则上面的算子定义依旧适用，下面我们对应在 Pytorch 运行的结果和我们刚实现的框架的结果：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#------------------- torch -------------------------|====================== Ours ========================</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x1 = torch.tensor([<span class="number">0.0140</span>, <span class="number">0.5773</span>, <span class="number">0.0469</span>],      &gt;&gt;&gt; x1 = Tensor(np.array([<span class="number">0.0140</span>, <span class="number">0.5773</span>, <span class="number">0.0469</span>]),</span><br><span class="line">        requires_grad=<span class="literal">True</span>)                                  requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x2 = torch.tensor([<span class="number">0.3232</span>, <span class="number">0.4903</span>, <span class="number">0.9395</span>],      &gt;&gt;&gt; x2 = Tensor(np.array([<span class="number">0.3232</span>, <span class="number">0.4903</span>, <span class="number">0.9395</span>]),</span><br><span class="line">        requires_grad=<span class="literal">True</span>)                                  requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v3 = torch.sin(x1)                               &gt;&gt;&gt; v3 = sin(x1)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v4 = torch.mul(x1, x2)                           &gt;&gt;&gt; v4 = mul(x1, x2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v5 = torch.add(v3, v4)                           &gt;&gt;&gt; v5 = add(v3, v4)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grad = torch.tensor([<span class="number">0.4948</span>, <span class="number">0.8746</span>, <span class="number">0.7076</span>])    &gt;&gt;&gt; grad = np.array([<span class="number">0.4948</span>, <span class="number">0.8746</span>, <span class="number">0.7076</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v5.backward(grad)                                &gt;&gt;&gt; v5.backward(grad)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x1.grad)                                   &gt;&gt;&gt; <span class="built_in">print</span>(x1.grad)</span><br><span class="line">tensor([<span class="number">0.6547</span>, <span class="number">1.1617</span>, <span class="number">1.3716</span>])                     tensor([<span class="number">0.65467087</span> <span class="number">1.16167806</span> <span class="number">1.37161212</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x2.grad)                                   &gt;&gt;&gt; <span class="built_in">print</span>(x2.grad)</span><br><span class="line">tensor([<span class="number">0.0069</span>, <span class="number">0.5049</span>, <span class="number">0.0332</span>])                     tensor([<span class="number">0.0069272</span>  <span class="number">0.50490658</span> <span class="number">0.03318644</span>])</span><br></pre></td></tr></table></figure></div>
<h2 id="小结"><a class="header-anchor" href="#小结"></a>小结</h2>
<p>本文中我们实现了一个自动微分（Automatic Differentiation）的框架。主要是 Tensor、
Operator 的定义，以及后向计算的引擎。</p>
<p>整体的实现和 PyTorch 的实现是比较类似的，但为了示例简单也做了些取舍。如
Pytorch 中 <code>Operator</code> 的第一个参数是 <code>ctx</code>，也鼓励算子把信息记录在 <code>ctx</code> 中，但我们是直接用 <code>self.x</code> 来记录；再如 PyTorch 中在计算结束后会把计算图销毁，我们没有做；再有 PyTorch 在 Tensor 中重载了一些基本操作（如 <code>+ - * /</code>），方便操作，但我们直接额外定义了 <code>add, mul</code> 等函数。等等等等。</p>
<p>总的来说，希望通过 AD 的简单实现，让大家认识到机器学习背后的一些原理，实际上也并没有特别复杂。当然我们也要认识到，能 Work 距离能在工业上使用，中间还隔了个太平洋。</p>
<h2 id="参考"><a class="header-anchor" href="#参考"></a>参考</h2>
<ul>
<li>CMU 的课程 Deep Learning Systems: Algorithms and Implementation
<ul>
<li><a href="https://www.youtube.com/watch?v=56WUlMEeAuA">Lecture 4 - Automatic Differentiation</a> AD 算法讲解</li>
<li><a href="https://www.youtube.com/watch?v=cNADlHfHQHg">Lecture 5 - Automatic Differentiation Implementation</a> AD 算法实现，着重讲解了诸如 Tensor，Operator 部分，图遍历的部分留做作业了。</li>
<li><a href="https://github.com/dlsyscourse/lecture4/blob/main/5_automatic_differentiation_implementation.ipynb">5_automatic_differentiation_implementation.ipynb</a> Lecture 5 的部分代码</li>
</ul>
</li>
<li><a href="https://www.52coding.com.cn/2019/05/05/PyTorch4/">PyTorch源码浅析(4)：Autograd</a> PyTorch 源码解析，版本比较老，但整体逻辑依旧适用</li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>除了邻接表外还有邻接矩阵的表示方法，参考<a href="https://en.wikipedia.org/wiki/Graph_theory#Tabular:_Graph_data_structures">维基百科</a> <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</div></div><script type="text/x-mathjax-config">
   MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    }); 
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="post-main post-comment"><div id="giscus_thread"></div><script src="https://giscus.app/client.js" data-repo="lotabout/lotabout.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkyMDU1NTQ0Nw==" data-category="Announcements" data-category-id="DIC_kwDOATmmt84ClmcD" data-mapping="" data-strict="" data-reactions-enabled="0" data-emit-metadata="" data-input-position="" data-theme="" data-lang="zh-CN" data-loading="" crossorigin="" async>
</script></div></article><link rel="stylesheet" type="text/css" href="/css/third-party/font-awesome/4.5.0/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/fancybox/2.1.5/jquery.fancybox.css"><script src="/js/third-party/jquery/2.0.3/jquery.min.js"></script><script src="/js/third-party/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=#{theme.google_analytics}"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-39956831-2');</script></body></html>