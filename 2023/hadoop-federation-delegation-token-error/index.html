<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="某一天 QA 报测试环境所有的 Spark 任务都鉴权失败。排查了好几天，分享下排查过程，大家就当看个故事。"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>BUG 定位：ZK 引起 HDFS 集群问题，导致 Spark 任务失败 | 三点水</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/books">Books</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-noise-link" href="/tags/BUG/" rel="tag">BUG</a><a class="post-tag-noise-link" href="/tags/delegation-token/" rel="tag">delegation token</a><a class="post-tag-noise-link" href="/tags/hadoop/" rel="tag">hadoop</a><a class="post-tag-noise-link" href="/tags/spark/" rel="tag">spark</a></div><div class="post-time">2023-01-13</div></div></div><div class="container post-header"><h1>BUG 定位：ZK 引起 HDFS 集群问题，导致 Spark 任务失败</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E6%8E%92%E6%9F%A5%E9%83%BD%E6%B2%A1%E9%97%AE%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">简单排查都没问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E4%B8%8D%E5%A4%9F-debug-%E6%9D%A5%E5%87%91"><span class="toc-number">2.</span> <span class="toc-text">知识不够，Debug 来凑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E4%BB%8E%E4%B8%8B%E6%89%8B-%E6%81%B6%E8%A1%A5%E7%9F%A5%E8%AF%86"><span class="toc-number">3.</span> <span class="toc-text">无从下手，恶补知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#client-or-server"><span class="toc-number">4.</span> <span class="toc-text">Client or Server?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-router-%E4%BC%9A%E5%87%BA%E9%94%99%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">为什么 router 会出错？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol></details></div><div class="container post-content"><p>某一天 QA 报测试环境所有的 Spark 任务都鉴权失败。排查了好几天，分享下排查过程，大家就当看个故事。</p>
<p>环境信息：</p>
<ul>
<li>Spark 3.1.2, 运行采用 Spark on k8s operator</li>
<li>数据存储在华为 FusionInsight 6.5.1, 并开启了联邦</li>
</ul>
<h2 id="简单排查都没问题"><a class="header-anchor" href="#简单排查都没问题"></a>简单排查都没问题</h2>
<p>Spark driver 看到的报错如下(省略了一些不重要的):</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Caused by: java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: &quot;spark-sql-xcjgz-43aabd8581a44cae-exec-4/10.244.55.49&quot;; destination host is: &quot;xxx-hdp02&quot;:25019; </span><br><span class="line">	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:776)</span><br><span class="line">        ...</span><br><span class="line">Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:688)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:651)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:738)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.call(Client.java:1452)</span><br><span class="line">	... 39 more</span><br><span class="line">Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]</span><br><span class="line">	at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:172)</span><br><span class="line">	at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:396)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:561)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:376)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:730)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:726)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)</span><br><span class="line">	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:726)</span><br><span class="line">	... 42 more</span><br></pre></td></tr></table></figure></div>
<p>由于环境之前都是正常的，大概率没有人动过环境。基于之前的经验，先排查一些常见的问题：</p>
<ul>
<li>确定 Spark Operator 需要的（<code>k get mutatingwebhookconfiguration</code>）存在。之前遇到过未知原因导致 webhook 消失，driver 没有 mount 上认证信息导致鉴权失败</li>
<li>确认 namespace 加了 label，可以被 webhook 监测到。之前遇到因为 namespace 没有相关 label，webhook 失效 mount 缺少认证信息，鉴权失败</li>
<li>尝试重启 Spark Operator Controller，无效。之前遇到过 controller 本身问题导致
mount 失效，可重启解决</li>
<li>确认 driver pod mount 进了 hadoop-conf, krb5.conf, keytab</li>
</ul>
<p>说明问题应该不是由 Spark Operator 引起的。然后怀疑是不是 mount 的鉴权信息有过变动：</p>
<ul>
<li>由于 driver 里没有 <code>kinit</code>，尝试把鉴权相关信息(<code>core-site.xml</code>,
<code>hdfs-site.xml</code>, <code>krb5.conf</code>, <code>keytab</code>) 放到另一台机器上，kinit 能成功。</li>
<li>在 kinit 的基础上，尝试 <code>hdfs mkdir</code> 和 <code>hdfs rmdir</code> 都能成功。</li>
</ul>
<p>初步认定 Hadoop 集群没问题<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>。</p>
<p>接着再排除代码问题，使用 driver 镜像在另一个环境（连的另一个 CDH 集群）能正常运行。</p>
<p>于是环境、集群、代码看起来都没有问题，那问题在哪呢？</p>
<h2 id="知识不够-debug-来凑"><a class="header-anchor" href="#知识不够-debug-来凑"></a>知识不够，Debug 来凑</h2>
<p>最开始还是怀疑环境有问题，怀疑 spark operator 依赖环境的某些东西被修改了。但实在不知道从何查起，于是考虑远程 debug。走了一些弯路，最后是这么操作的：</p>
<ol>
<li>修改提交的 spark application，加上参数
<code>spark.driver.extraJavaOptions=&quot;-agentlib:jdwp=transport=dt_socket,server=y, suspend=y,address=5005&quot;</code>，这样 driver 启动后就会开启 5005 端口等待 debug</li>
<li>之后通过 <code>kubectl port-forward --address 0.0.0.0 &lt;driver pod&gt; 5005: 5005</code> 来开启宿主机到 pod 的流量转发</li>
<li>开启 IDEA 远程 debug，连上宿主机的 5005 端口。由于报错的是 hadoop 相关的，随便开了一个项目引入 hadoop 依赖，打断点就能用了。</li>
</ol>
<p>断点打在 <code>UserGroupInformation.doAs</code> 上。发现 driver 调用时的用户信息都正常。再通过添加 <code>spark.executor.extracJavaOptions</code> 参数来 debug executor（记得把
executor数调成 1）。结果发现 executor 调用 <code>doAs</code> 时，使用的用户名是
<code>root</code>（预期是 <code>work</code>），鉴权模式是 <code>SIMPLE</code>（预期是 <code>KERBEROS</code>）。</p>
<p>这妥妥的是 executor pod 创建的问题呀。于是开始排查 executor，发现 executor 的环境变量 <code>SPARK_USER=root</code>，同时它没有mount krb5.conf 和 keytab，难道发现了
root cause?</p>
<p>可惜几番折腾后都不生效。最后对比运行成功的环境，发现运行成功的 exeucotr 环境变量也是一样的，也没有 mount 任何鉴权相关的信息。</p>
<h2 id="无从下手-恶补知识"><a class="header-anchor" href="#无从下手-恶补知识"></a>无从下手，恶补知识</h2>
<p>既然没有任何鉴权相关的信息，executor 里是怎么鉴权的？涉及到知识盲区，怎么办？</p>
<p>下载 Spark 3.1.1 代码，但代码太多又无从看起，于是上网搜索相关 Feature 的PR，找到下面几个信息：</p>
<ul>
<li><a href="https://github.com/apache/spark/pull/21669">https://github.com/apache/spark/pull/21669</a> 增加 kerberos support for k8s</li>
<li><a href="https://docs.google.com/document/d/1RBnXD9jMDjGonOdKJ2bA1lN4AAV_1RwpU_ewFuCNWKg/edit">https://docs.google.com/document/d/1RBnXD9jMDjGonOdKJ2bA1lN4AAV_1RwpU_ewFuCNWKg/edit</a> spark kerberos support 的设计文档</li>
<li><a href="https://github.com/apache/spark/pull/22911">https://github.com/apache/spark/pull/22911</a> 增加了 client mode 的支持</li>
</ul>
<p>最重要的是最后这个链接，这个 PR 的留言里有这样的信息：</p>
<blockquote>
<p>In either of those cases, the driver code will handle delegation tokens: in
cluster mode by creating a secret and stashing them, in client mode by using
existing mechanisms to <strong>send DTs to executors</strong>.</p>
</blockquote>
<p>说明鉴权都是 driver 做的，而 executor 会从 driver 拿到 delegation token。</p>
<p>不过 “delegation token” 又是啥玩意？大概搜到它是 Hadoop 发的 token，目标是减少鉴权压力，一般在 Map Reduce, Spark 这些有多个 worker 要访问 HDFS 的时候使用。但这些信息并没有本质帮助。</p>
<p>好在这样有了 debug 的头绪。一开始尝试使用 spark 的源码进行 remote debug，发现有许多问题搞不定。于是尝试直接下载 driver 里的所有 jar 包，导入到一个空
project 中，由于是 scala 直接依赖 jar 包不好单步，于是再下载对应 jar 包的
sources.jar，就可以在 IDEA 里打断点单步执行了。</p>
<p>同时 debug 成功和失败环境里的 executor，在对比一些步骤的变量后，终于发现问题所在：executor 在执行下面代码时，失败的环境里获取到的 token 是空。</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">cfg.hadoopDelegationCreds.foreach &#123; tokens =&gt;</span><br><span class="line">  <span class="type">SparkHadoopUtil</span>.get.addDelegationTokens(tokens, driverConf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>考虑到 DT 是从 driver 获取的，看来是 driver 里存储的 delegation token 本来就是空的。另外回过头来发现 driver 的日志里有这么一句日志：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">23/01/08 21:03:31 INFO DFSClient: Cannot get delegation token from work</span><br></pre></td></tr></table></figure></div>
<p>为什么 driver token 是空？继续 debug driver 发现 driver 在获取 delegation
token 时返回的是 null: <code>FileSystem.collectDelegationTokens</code>。最终缩小到最小的复现代码：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">var</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">org</span>.apache.hadoop.conf.Configuration();</span><br><span class="line">conf.addResource(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;./core-site.xml&quot;</span>));</span><br><span class="line">conf.addResource(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;./hdfs-site.xml&quot;</span>));</span><br><span class="line">System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;./krb5.conf&quot;</span>);</span><br><span class="line">UserGroupInformation.setConfiguration(conf);</span><br><span class="line">UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;work&quot;</span>, <span class="string">&quot;./user.keytab&quot;</span>);</span><br><span class="line"><span class="type">var</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(URI.create(<span class="string">&quot;hdfs:///&quot;</span>), conf);</span><br><span class="line"><span class="type">var</span> <span class="variable">creds</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Credentials</span>();</span><br><span class="line">fs.addDelegationTokens(<span class="string">&quot;work&quot;</span>, creds);</span><br><span class="line">Assertions.assertFalse(creds.getAllTokens().isEmpty());</span><br></pre></td></tr></table></figure></div>
<h2 id="client-or-server"><a class="header-anchor" href="#client-or-server"></a>Client or Server?</h2>
<p>是 hadoop client 有 BUG？还是 hadoop server 有问题？</p>
<p>查代码看到 delegation token 是 namenode 创建的。于是上集群的 namenode，用
arthas 监控 <code>FSNamesystem.getDelegationToken</code> 方法。</p>
<p>结果……执行复现代码，发现没有输出，尽管同时监控了所有的 4 个 namenode，没有任何一个有输出。然而在正常的环境里是有输出的。难道是 Client 有 BUG 没把请求发出去？</p>
<p>开始用 wireshark 抓包，发现还是有请求包发出的，当然因为是 RPC，内容看不出来，但 namenode 的 arthas 就是没有输出……最后在对比正常和错误环境的请求包，突然发现错误环境连接的是 <code>25019</code> 端口，这又是啥端口？</p>
<p>在集群上通过 <code>netstat -natp</code> 找到了进程，进程的命令显示它是集群的 <code>router</code> 角色。不管是 arthas 还是它的日志（如下），都发现它才是罪魁祸首：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2023-01-09 20:09:59,312 | WARN  | IPC Server handler 43 on 25019 | trying to get DT with no secret manager running | RouterSecurityManager.java:124</span><br><span class="line">2023-01-09 20:09:59,391 | WARN  | IPC Server handler 39 on 25019 | trying to get DT with no secret manager running | RouterSecurityManager.java:124</span><br></pre></td></tr></table></figure></div>
<p>其实之前在看 hadoop 相关代码时就注意到如果 server 出错应该要有这个日志，但在
namenode 日志里没有找到。现在看到这个日志，基本确定就是它的问题。最后重启
router 之后发现世界和平了。</p>
<h2 id="为什么-router-会出错？"><a class="header-anchor" href="#为什么-router-会出错？"></a>为什么 router 会出错？</h2>
<p>Router 生成 <a href="https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java">getDelegationToken</a>
的逻辑如下所示，通过 arthas 发现是因为 <code>dtSecretManager.isRunning</code> 判断失败。再追代码发现 <code>isRunning</code> 失败的唯一可能就是调用了
<code>AbstractDelegationTokenSecretManager::stopThreads</code>，但是 <code>stopThreads</code> 只有停止 router的时候才会调用，与当前的现象不相符。</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Token&lt;DelegationTokenIdentifier&gt; <span class="title function_">getDelegationToken</span><span class="params">(Text renewer)</span></span><br><span class="line">    <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  LOG.debug(<span class="string">&quot;Generate delegation token with renewer &quot;</span> + renewer);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">String</span> <span class="variable">operationName</span> <span class="operator">=</span> <span class="string">&quot;getDelegationToken&quot;</span>;</span><br><span class="line">  <span class="type">boolean</span> <span class="variable">success</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">  <span class="type">String</span> <span class="variable">tokenId</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">  Token&lt;DelegationTokenIdentifier&gt; token;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (!isAllowedDelegationTokenOp()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(</span><br><span class="line">          <span class="string">&quot;Delegation Token can be issued only &quot;</span> +</span><br><span class="line">              <span class="string">&quot;with kerberos or web authentication&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (dtSecretManager == <span class="literal">null</span> || !dtSecretManager.isRunning()) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;trying to get DT with no secret manager running&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></div>
<p>可惜的是 <code>stopThreads</code> 的调用链路并不会输出日志。只能尝试人肉看一看从最后一次重启，到出问题之间的日志，开头结尾如下：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2022-12-29 18:54:18,187 | INFO  | pool-1-thread-1 | Stopping security manager | RouterSecurityManager.java:62</span><br><span class="line">2022-12-29 21:14:37,456 | WARN  | IPC Server handler 5 on 25019 | trying to get DT with no secret manager running | RouterSecurityManager.java:124</span><br></pre></td></tr></table></figure></div>
<p>看了不久发现，在创建 secret manager，调用 <code>startThreads</code> 时因为 ZK 的原因有报错：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2022-12-29 21:13:06,167 | ERROR | main | Error starting threads for zkDelegationTokens  | ZKDelegationTokenSecretManagerImpl.java:48</span><br><span class="line">java.io.IOException: Could not start Sequence Counter</span><br><span class="line">	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.startThreads(ZKDelegationTokenSecretManager.java:324)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl.&lt;init&gt;(ZKDelegationTokenSecretManagerImpl.java:46)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.FederationUtil.newSecretManager(FederationUtil.java:214)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager.&lt;init&gt;(RouterSecurityManager.java:54)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.&lt;init&gt;(RouterRpcServer.java:262)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.Router.createRpcServer(Router.java:385)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.Router.serviceInit(Router.java:194)</span><br><span class="line">	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.federation.router.DFSRouter.main(DFSRouter.java:69)</span><br><span class="line">Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hdfs-federation/RouterZkDtSecretManager/ZKDTSMRoot/ZKDTSMSeqNumRoot</span><br><span class="line">	at org.apache.zookeeper.KeeperException.create(KeeperException.java:104)</span><br><span class="line">	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)</span><br><span class="line">	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1480)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:740)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:723)</span><br><span class="line">	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:720)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:484)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:474)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:349)</span><br><span class="line">	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:291)</span><br><span class="line">	at org.apache.curator.framework.recipes.shared.SharedValue.start(SharedValue.java:229)</span><br><span class="line">	at org.apache.curator.framework.recipes.shared.SharedCount.start(SharedCount.java:155)</span><br><span class="line">	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.startThreads(ZKDelegationTokenSecretManager.java:321)</span><br><span class="line">	... 12 more</span><br><span class="line">2022-12-29 21:13:06,168 | INFO  | main | Secret manager instance created | FederationUtil.java:215</span><br></pre></td></tr></table></figure></div>
<p>但如果创建的时候就失败了，按<a href="https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java#L54">代码逻辑</a>，启动的时候应该失败：</p>
<div class="noise-code-block" style="--code-block-max-height:inherit"><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">RouterSecurityManager</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="type">AuthenticationMethod</span> <span class="variable">authMethodConfigured</span> <span class="operator">=</span></span><br><span class="line">      SecurityUtil.getAuthenticationMethod(conf);</span><br><span class="line">  <span class="type">AuthenticationMethod</span> <span class="variable">authMethodToInit</span> <span class="operator">=</span></span><br><span class="line">      AuthenticationMethod.KERBEROS;</span><br><span class="line">  <span class="keyword">if</span> (authMethodConfigured.equals(authMethodToInit)) &#123;</span><br><span class="line">    <span class="built_in">this</span>.dtSecretManager = FederationUtil.newSecretManager(conf);</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.dtSecretManager == <span class="literal">null</span> || !<span class="built_in">this</span>.dtSecretManager.isRunning()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(<span class="string">&quot;Failed to create SecretManager&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>最后发现看的代码和集群的版本不一致，FI 6.5.1 是基于 hadoop 3.1.1 版本，但是
3.1.1 版本代码里并没有 <code>RouterSecurityManager</code>，于是拉到集群里的 jar 包反编译，发现，FI 实现的 RouterSecurityManager 没有任何的校验：</p>
<p><img src="2023-01-fi-router-security-manager-constructor.png" alt=""></p>
<p>而开源的 hadoop 在创建完后是有校验的，也找到了相关的修改<a href="https://github.com/apache/hadoop/commit/524b553a5f1c10bf41c723302cf42b592ffa1631">commit</a> 。
<img src="2023-01-open-sourcing-hadoop-router-security-manager-constructor.png" alt=""></p>
<h2 id="小结"><a class="header-anchor" href="#小结"></a>小结</h2>
<p>最终的问题链路是：</p>
<ol>
<li>FI 集群 Router 重启，启动过程需要创建 secret manager，具体的实现是基于 ZK的</li>
<li>由于某些原因在启动 <code>ZKDelegationTokenSecretManagerImpl</code> 连接 ZK 失败，导致相关的服务启动失败</li>
<li>但是 <code>RouterSecurityManager</code> 并没有对 secret manager 的启动状态做校验（后续版本修复），仍然继续运行</li>
<li>在实际创建 delegation token 时，由于 secret manager 的状态异常，创建的请求也失败</li>
<li>Spark Driver 在获取 delegation token 时得到一个空值</li>
<li>Spark Executor 需要访问 HDFS 时，会从 Driver 的 Resource Manager 中获取
profile，其中包含 driver 获取的 token，由于 token 是空的，于是鉴权失败。</li>
</ol>
<p>另外几点感想：</p>
<ul>
<li>所幸问题是发生在测试环境，要发生在客户环境，估计一万年都查不出来（当然任务肯定也会失败）。</li>
<li>另外虽然看代码很重要，但还是应该补齐相关模块的理论知识，这样事半功倍。</li>
<li>重启大法好</li>
</ul>
<h2 id="参考"><a class="header-anchor" href="#参考"></a>参考</h2>
<ul>
<li><a href="https://blog.cloudera.com/hadoop-delegation-tokens-explained/">https://blog.cloudera.com/hadoop-delegation-tokens-explained/</a> Hadoop
delegation token 介绍，比较偏使用而非实现原理</li>
<li><a href="https://docs.google.com/document/d/1RBnXD9jMDjGonOdKJ2bA1lN4AAV_1RwpU_ewFuCNWKg/edit">https://docs.google.com/document/d/1RBnXD9jMDjGonOdKJ2bA1lN4AAV_1RwpU_ewFuCNWKg/edit</a> spark on k8s kerberos 支持的设计文档，跟实现还是有点差距</li>
<li><a href="https://medium.com/agile-lab-engineering/spark-remote-debugging-371a1a8c44a8">https://medium.com/agile-lab-engineering/spark-remote-debugging-371a1a8c44a8</a>
spark driver/executor 远程 debug 的方法</li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>这里犯了一个错误，就是通过 kinit 成功推断集群正常。这里因为不了解 hadoop 额外的一些机制导致的，不太好避免 <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</div></div><div class="post-main post-comment"><div id="giscus_thread"></div><script src="https://giscus.app/client.js" data-repo="lotabout/lotabout.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkyMDU1NTQ0Nw==" data-category="Announcements" data-category-id="DIC_kwDOATmmt84ClmcD" data-mapping="" data-strict="" data-reactions-enabled="0" data-emit-metadata="" data-input-position="" data-theme="" data-lang="zh-CN" data-loading="" crossorigin="" async>
</script></div></article><link rel="stylesheet" type="text/css" href="/css/third-party/font-awesome/4.5.0/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcdn.net/ajax/libs/fancybox/2.1.5/jquery.fancybox.css"><script src="/js/third-party/jquery/2.0.3/jquery.min.js"></script><script src="/js/third-party/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=#{theme.google_analytics}"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-39956831-2');</script></body></html>